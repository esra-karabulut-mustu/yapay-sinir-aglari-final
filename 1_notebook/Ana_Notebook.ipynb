{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ScTm94mT32s"
      },
      "source": [
        "# ISIC 2018 Klasifikasyon Pipeline (Colab Versiyonu)\n",
        "\n",
        "Bu notebook, Google Colab üzerinde uçtan uca çalışacak şekilde tasarlanmıştır.\n",
        "Otomatik olarak:\n",
        "1. Kütüphaneleri kurar.\n",
        "2. Kaggle API ile veriyi indirir.\n",
        "3. Gerekli modülleri oluşturur.\n",
        "4. Eğitimi ve değerlendirmeyi başlatır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Had9BeIET32w"
      },
      "source": [
        "## 1. Hazırlık ve Kurulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKdyf-aOT32x",
        "outputId": "fa58e754-f2cb-4ea6-91d9-caac471b8ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m51.2/52.5 kB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m993.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Gerekli kütüphaneleri yükle\n",
        "!pip install -q tensorflow pandas matplotlib seaborn scikit-learn kagglehub tf-keras-vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njj1-QOtT320",
        "outputId": "d4c41697-31dd-462d-8602-608f20bac161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'skin-cancer9-classesisic' dataset.\n",
            "Path to dataset files: /kaggle/input/skin-cancer9-classesisic\n"
          ]
        }
      ],
      "source": [
        "# Veri Setini İndir (kagglehub ile)\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nodoubttome/skin-cancer9-classesisic\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Path'i global değişkende sakla\n",
        "DATASET_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CMgIzMNsT321"
      },
      "outputs": [],
      "source": [
        "# Kaynak kod klasörünü oluştur\n",
        "!mkdir -p src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkMYXE8jT322",
        "outputId": "9c93c2cd-7204-441d-a6dc-00031e273095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/__init__.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification Package\n",
        "\"\"\"\n",
        "from . import config\n",
        "from . import data_loader\n",
        "from . import models\n",
        "from . import training\n",
        "from . import evaluation\n",
        "from . import gradcam\n",
        "\n",
        "__version__ = \"0.1.0\"\n",
        "__all__ = [\"config\", \"data_loader\", \"models\", \"training\", \"evaluation\", \"gradcam\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfYECGCoT323",
        "outputId": "d83f714e-95df-4903-8ce0-97773308bc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/config.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Configuration Module\n",
        "\"\"\"\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = '/content'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'isic_data')\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"Train\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"Test\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
        "MODELS_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
        "FIGURES_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
        "GRADCAM_DIR = os.path.join(OUTPUT_DIR, \"gradcam\")\n",
        "REPORTS_DIR = os.path.join(OUTPUT_DIR, \"reports\")\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "IMG_SHAPE = (224, 224, 3)\n",
        "\n",
        "# Data split ratios\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "REDUCE_LR_PATIENCE = 5\n",
        "REDUCE_LR_FACTOR = 0.2\n",
        "MIN_LR = 1e-6\n",
        "\n",
        "# Learning rates\n",
        "SCRATCH_LR = 1e-3\n",
        "TL_FREEZE_LR = 1e-3\n",
        "TL_FINETUNE_LR = 1e-5\n",
        "\n",
        "# Fine-tuning: unfreeze last 25% of layers\n",
        "FINETUNE_RATIO = 0.25\n",
        "\n",
        "# Augmentation parameters\n",
        "ROTATION_RANGE = 15  # degrees\n",
        "ZOOM_RANGE = 0.1\n",
        "BRIGHTNESS_RANGE = 0.1\n",
        "HORIZONTAL_FLIP = True\n",
        "VERTICAL_FLIP = True\n",
        "\n",
        "# Class mapping (will be populated after dataset analysis)\n",
        "CLASS_NAMES = []\n",
        "BINARY_LABELS = {}\n",
        "\n",
        "\n",
        "def set_seed(seed: int = SEED):\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed)\n",
        "        # Set deterministic operations (may impact performance)\n",
        "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "\n",
        "def ensure_dirs():\n",
        "    \"\"\"Create output directories if they don't exist.\"\"\"\n",
        "    for dir_path in [MODELS_DIR, FIGURES_DIR, GRADCAM_DIR, REPORTS_DIR]:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# Initialize\n",
        "set_seed()\n",
        "ensure_dirs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2L4a2ujT325",
        "outputId": "d2d5bc0f-820f-456f-f52b-8fd991fbdf7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/data_loader.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Data Loading Module\n",
        "\"\"\"\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from . import config\n",
        "\n",
        "\n",
        "def analyze_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyze the dataset and return class distribution.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with class names and sample counts\n",
        "    \"\"\"\n",
        "    class_counts = {}\n",
        "\n",
        "    for class_name in os.listdir(config.TRAIN_DIR):\n",
        "        class_path = os.path.join(config.TRAIN_DIR, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            count = len([f for f in os.listdir(class_path)\n",
        "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            class_counts[class_name] = count\n",
        "\n",
        "    df = pd.DataFrame([\n",
        "        {'class_name': k, 'count': v}\n",
        "        for k, v in class_counts.items()\n",
        "    ]).sort_values('count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ISIC 2018 Dataset - Class Distribution\")\n",
        "    print(\"=\"*60)\n",
        "    print(df.to_string(index=False))\n",
        "    print(f\"\\nTotal samples: {df['count'].sum()}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_top_two_classes() -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Get the two classes with the highest sample counts.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (class_0_name, class_1_name)\n",
        "    \"\"\"\n",
        "    df = analyze_dataset()\n",
        "    top_two = df.head(2)['class_name'].tolist()\n",
        "\n",
        "    print(f\"\\nSelected classes for binary classification:\")\n",
        "    print(f\"  Class 0: {top_two[0]}\")\n",
        "    print(f\"  Class 1: {top_two[1]}\")\n",
        "\n",
        "    return top_two[0], top_two[1]\n",
        "\n",
        "\n",
        "def load_image_paths_and_labels(class_0: str, class_1: str) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Load image paths and binary labels for the selected two classes.\n",
        "\n",
        "    Args:\n",
        "        class_0: Name of class to be labeled as 0\n",
        "        class_1: Name of class to be labeled as 1\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (image_paths, labels)\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Load class 0\n",
        "    class_0_path = os.path.join(config.TRAIN_DIR, class_0)\n",
        "    for img_name in os.listdir(class_0_path):\n",
        "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_paths.append(os.path.join(class_0_path, img_name))\n",
        "            labels.append(0)\n",
        "\n",
        "    # Load class 1\n",
        "    class_1_path = os.path.join(config.TRAIN_DIR, class_1)\n",
        "    for img_name in os.listdir(class_1_path):\n",
        "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_paths.append(os.path.join(class_1_path, img_name))\n",
        "            labels.append(1)\n",
        "\n",
        "    print(f\"\\nLoaded {len(image_paths)} images:\")\n",
        "    print(f\"  Class 0 ({class_0}): {labels.count(0)}\")\n",
        "    print(f\"  Class 1 ({class_1}): {labels.count(1)}\")\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "\n",
        "def create_stratified_split(\n",
        "    image_paths: List[str],\n",
        "    labels: List[int]\n",
        ") -> Dict[str, Tuple[List[str], List[int]]]:\n",
        "    \"\"\"\n",
        "    Create stratified train/validation/test splits.\n",
        "\n",
        "    Args:\n",
        "        image_paths: List of image file paths\n",
        "        labels: List of corresponding labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'train', 'val', 'test' keys containing (paths, labels) tuples\n",
        "    \"\"\"\n",
        "    # First split: train+val vs test\n",
        "    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
        "        image_paths, labels,\n",
        "        test_size=config.TEST_RATIO,\n",
        "        stratify=labels,\n",
        "        random_state=config.SEED\n",
        "    )\n",
        "\n",
        "    # Second split: train vs val\n",
        "    val_ratio_adjusted = config.VAL_RATIO / (config.TRAIN_RATIO + config.VAL_RATIO)\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        train_val_paths, train_val_labels,\n",
        "        test_size=val_ratio_adjusted,\n",
        "        stratify=train_val_labels,\n",
        "        random_state=config.SEED\n",
        "    )\n",
        "\n",
        "    splits = {\n",
        "        'train': (train_paths, train_labels),\n",
        "        'val': (val_paths, val_labels),\n",
        "        'test': (test_paths, test_labels)\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Stratified Split Results\")\n",
        "    print(\"=\"*60)\n",
        "    for split_name, (paths, lbls) in splits.items():\n",
        "        class_dist = Counter(lbls)\n",
        "        print(f\"{split_name.upper():>10}: {len(paths):>4} samples | \"\n",
        "              f\"Class 0: {class_dist[0]:>3} | Class 1: {class_dist[1]:>3}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "def create_augmentation_layer() -> tf.keras.Sequential:\n",
        "    \"\"\"\n",
        "    Create data augmentation layer for training.\n",
        "\n",
        "    Returns:\n",
        "        Sequential model containing augmentation layers\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        tf.keras.layers.RandomRotation(config.ROTATION_RANGE / 360.0),\n",
        "        tf.keras.layers.RandomZoom(config.ZOOM_RANGE),\n",
        "        tf.keras.layers.RandomBrightness(config.BRIGHTNESS_RANGE),\n",
        "        tf.keras.layers.RandomContrast(config.BRIGHTNESS_RANGE),\n",
        "    ], name='augmentation')\n",
        "\n",
        "\n",
        "def preprocess_image(path: str, label: int) -> Tuple[tf.Tensor, int]:\n",
        "    \"\"\"\n",
        "    Load and preprocess a single image.\n",
        "\n",
        "    Args:\n",
        "        path: Image file path\n",
        "        label: Image label\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (preprocessed_image, label)\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Resize\n",
        "    img = tf.image.resize(img, config.IMG_SIZE)\n",
        "    # Normalize to [0, 1]\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    return img, label\n",
        "\n",
        "\n",
        "def create_dataset(\n",
        "    paths: List[str],\n",
        "    labels: List[int],\n",
        "    is_training: bool = False,\n",
        "    batch_size: int = None\n",
        ") -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Create a tf.data.Dataset pipeline.\n",
        "\n",
        "    Args:\n",
        "        paths: List of image paths\n",
        "        labels: List of labels\n",
        "        is_training: Whether to apply augmentation and shuffle\n",
        "        batch_size: Batch size (uses config default if None)\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = config.BATCH_SIZE\n",
        "\n",
        "    # Create dataset from tensors\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "\n",
        "    # Shuffle if training\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=len(paths), seed=config.SEED)\n",
        "\n",
        "    # Map preprocessing (parallel)\n",
        "    dataset = dataset.map(\n",
        "        preprocess_image,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Batch\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Apply augmentation if training (after batching)\n",
        "    if is_training:\n",
        "        augmentation = create_augmentation_layer()\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (augmentation(x, training=True), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "    # Cache and prefetch\n",
        "    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def visualize_augmentation(\n",
        "    dataset: tf.data.Dataset,\n",
        "    class_names: Tuple[str, str],\n",
        "    num_samples: int = 6,\n",
        "    save_path: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize augmented samples.\n",
        "\n",
        "    Args:\n",
        "        dataset: Training dataset with augmentation\n",
        "        class_names: Tuple of class names\n",
        "        num_samples: Number of samples to visualize\n",
        "        save_path: Path to save the figure\n",
        "    \"\"\"\n",
        "    # Get one batch\n",
        "    for images, labels in dataset.take(1):\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "        fig.suptitle('Data Augmentation Examples', fontsize=14)\n",
        "\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            if i < len(images):\n",
        "                ax.imshow(images[i].numpy())\n",
        "                ax.set_title(f'Class: {class_names[labels[i].numpy()]}')\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"Augmentation examples saved to: {save_path}\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def prepare_data() -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, Tuple[str, str], Dict]:\n",
        "    \"\"\"\n",
        "    Main function to prepare all data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_ds, val_ds, test_ds, class_names, split_info)\n",
        "    \"\"\"\n",
        "    # Get top two classes\n",
        "    class_0, class_1 = get_top_two_classes()\n",
        "    class_names = (class_0, class_1)\n",
        "\n",
        "    # Load paths and labels\n",
        "    image_paths, labels = load_image_paths_and_labels(class_0, class_1)\n",
        "\n",
        "    # Create stratified splits\n",
        "    splits = create_stratified_split(image_paths, labels)\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = create_dataset(\n",
        "        splits['train'][0], splits['train'][1],\n",
        "        is_training=True\n",
        "    )\n",
        "    val_ds = create_dataset(\n",
        "        splits['val'][0], splits['val'][1],\n",
        "        is_training=False\n",
        "    )\n",
        "    test_ds = create_dataset(\n",
        "        splits['test'][0], splits['test'][1],\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    # Save augmentation examples\n",
        "    visualize_augmentation(\n",
        "        train_ds, class_names,\n",
        "        save_path=os.path.join(config.FIGURES_DIR, 'augmentation_examples.png')\n",
        "    )\n",
        "\n",
        "    # Prepare split info for reporting\n",
        "    split_info = {\n",
        "        'train': {'total': len(splits['train'][0]), 'class_0': splits['train'][1].count(0), 'class_1': splits['train'][1].count(1)},\n",
        "        'val': {'total': len(splits['val'][0]), 'class_0': splits['val'][1].count(0), 'class_1': splits['val'][1].count(1)},\n",
        "        'test': {'total': len(splits['test'][0]), 'class_0': splits['test'][1].count(0), 'class_1': splits['test'][1].count(1)},\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "    # Store test paths for later Grad-CAM analysis\n",
        "    split_info['test_paths'] = splits['test'][0]\n",
        "    split_info['test_labels'] = splits['test'][1]\n",
        "\n",
        "    return train_ds, val_ds, test_ds, class_names, split_info\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the data loading\n",
        "    train_ds, val_ds, test_ds, class_names, split_info = prepare_data()\n",
        "\n",
        "    print(\"\\nDataset shapes:\")\n",
        "    for x, y in train_ds.take(1):\n",
        "        print(f\"  Batch shape: {x.shape}\")\n",
        "        print(f\"  Labels shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHyKLsh2T327",
        "outputId": "69cc553e-1527-4a30-fcae-cd4dcca3ef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/models.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Models Module\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, Model\n",
        "\n",
        "from . import config\n",
        "\n",
        "\n",
        "def create_scratch_cnn(input_shape: tuple = None) -> Model:\n",
        "    \"\"\"\n",
        "    Create a Scratch CNN model as specified in the project requirements.\n",
        "\n",
        "    Architecture:\n",
        "    - Block 1: Conv(32) x2 + MaxPool + Dropout\n",
        "    - Block 2: Conv(64) x2 + MaxPool + Dropout\n",
        "    - Block 3: Conv(128) x2 + MaxPool + Dropout\n",
        "    - Block 4: Conv(256) + MaxPool + Dropout\n",
        "    - Head: GAP + Dense(128) + Dense(1)\n",
        "\n",
        "    Args:\n",
        "        input_shape: Input image shape (default: from config)\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    if input_shape is None:\n",
        "        input_shape = config.IMG_SHAPE\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape, name='input')\n",
        "\n",
        "    # Block 1\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', name='block1_conv1')(inputs)\n",
        "    x = layers.BatchNormalization(name='block1_bn1')(x)\n",
        "    x = layers.ReLU(name='block1_relu1')(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', name='block1_conv2')(x)\n",
        "    x = layers.BatchNormalization(name='block1_bn2')(x)\n",
        "    x = layers.ReLU(name='block1_relu2')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='block1_pool')(x)\n",
        "    x = layers.Dropout(0.25, name='block1_dropout')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', name='block2_conv1')(x)\n",
        "    x = layers.BatchNormalization(name='block2_bn1')(x)\n",
        "    x = layers.ReLU(name='block2_relu1')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', name='block2_conv2')(x)\n",
        "    x = layers.BatchNormalization(name='block2_bn2')(x)\n",
        "    x = layers.ReLU(name='block2_relu2')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='block2_pool')(x)\n",
        "    x = layers.Dropout(0.25, name='block2_dropout')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', name='block3_conv1')(x)\n",
        "    x = layers.BatchNormalization(name='block3_bn1')(x)\n",
        "    x = layers.ReLU(name='block3_relu1')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', name='block3_conv2')(x)\n",
        "    x = layers.BatchNormalization(name='block3_bn2')(x)\n",
        "    x = layers.ReLU(name='block3_relu2')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='block3_pool')(x)\n",
        "    x = layers.Dropout(0.30, name='block3_dropout')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same', name='block4_conv')(x)\n",
        "    x = layers.BatchNormalization(name='block4_bn')(x)\n",
        "    x = layers.ReLU(name='block4_relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name='block4_pool')(x)\n",
        "    x = layers.Dropout(0.35, name='block4_dropout')(x)\n",
        "\n",
        "    # Head\n",
        "    x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
        "    x = layers.Dense(128, name='fc1')(x)\n",
        "    x = layers.ReLU(name='fc1_relu')(x)\n",
        "    x = layers.Dropout(0.5, name='fc1_dropout')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='ScratchCNN')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_mobilenet_model(input_shape: tuple = None, trainable: bool = False) -> tuple:\n",
        "    \"\"\"\n",
        "    Create a MobileNetV2 transfer learning model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Input image shape\n",
        "        trainable: Whether base model is trainable (False for freeze phase)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, base_model)\n",
        "    \"\"\"\n",
        "    if input_shape is None:\n",
        "        input_shape = config.IMG_SHAPE\n",
        "\n",
        "    # Load pretrained MobileNetV2\n",
        "    base_model = keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    # Build model with custom head\n",
        "    inputs = layers.Input(shape=input_shape, name='input')\n",
        "\n",
        "    # Preprocessing for MobileNet\n",
        "    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "    x = base_model(x, training=trainable)\n",
        "\n",
        "    # Custom head\n",
        "    x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
        "    x = layers.Dropout(0.4, name='head_dropout')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='MobileNetV2_TL')\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "\n",
        "def create_efficientnet_model(input_shape: tuple = None, trainable: bool = False) -> tuple:\n",
        "    \"\"\"\n",
        "    Create an EfficientNetB0 transfer learning model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Input image shape\n",
        "        trainable: Whether base model is trainable\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, base_model)\n",
        "    \"\"\"\n",
        "    if input_shape is None:\n",
        "        input_shape = config.IMG_SHAPE\n",
        "\n",
        "    # Load pretrained EfficientNetB0\n",
        "    base_model = keras.applications.EfficientNetB0(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    # Build model with custom head\n",
        "    inputs = layers.Input(shape=input_shape, name='input')\n",
        "\n",
        "    # EfficientNet has built-in preprocessing\n",
        "    x = base_model(inputs, training=trainable)\n",
        "\n",
        "    # Custom head\n",
        "    x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
        "    x = layers.Dropout(0.4, name='head_dropout')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_TL')\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "\n",
        "def unfreeze_top_layers(base_model: Model, ratio: float = None) -> int:\n",
        "    \"\"\"\n",
        "    Unfreeze the top percentage of layers for fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        base_model: The base model to unfreeze\n",
        "        ratio: Ratio of layers to unfreeze (default: from config)\n",
        "\n",
        "    Returns:\n",
        "        Number of layers unfrozen\n",
        "    \"\"\"\n",
        "    if ratio is None:\n",
        "        ratio = config.FINETUNE_RATIO\n",
        "\n",
        "    total_layers = len(base_model.layers)\n",
        "    fine_tune_from = int((1 - ratio) * total_layers)\n",
        "\n",
        "    # Keep BatchNorm layers frozen (recommended for fine-tuning)\n",
        "    unfrozen_count = 0\n",
        "    for i, layer in enumerate(base_model.layers):\n",
        "        if i >= fine_tune_from:\n",
        "            if not isinstance(layer, layers.BatchNormalization):\n",
        "                layer.trainable = True\n",
        "                unfrozen_count += 1\n",
        "            # Keep BatchNorm frozen for stability\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "    print(f\"\\nFine-tuning configuration:\")\n",
        "    print(f\"  Total layers: {total_layers}\")\n",
        "    print(f\"  Frozen layers: {fine_tune_from}\")\n",
        "    print(f\"  Unfrozen layers: {unfrozen_count}\")\n",
        "    print(f\"  BatchNorm layers: kept frozen for stability\")\n",
        "\n",
        "    return unfrozen_count\n",
        "\n",
        "\n",
        "def compile_model(\n",
        "    model: Model,\n",
        "    learning_rate: float,\n",
        "    show_summary: bool = True\n",
        ") -> Model:\n",
        "    \"\"\"\n",
        "    Compile a model with BinaryCrossentropy and Adam optimizer.\n",
        "\n",
        "    Args:\n",
        "        model: Keras model to compile\n",
        "        learning_rate: Learning rate for Adam optimizer\n",
        "        show_summary: Whether to print model summary\n",
        "\n",
        "    Returns:\n",
        "        Compiled model\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[\n",
        "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "            keras.metrics.AUC(name='auc'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if show_summary:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Model: {model.name}\")\n",
        "        print(f\"Learning Rate: {learning_rate}\")\n",
        "        print('='*60)\n",
        "        model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model_info(model: Model) -> dict:\n",
        "    \"\"\"\n",
        "    Get model information for reporting.\n",
        "\n",
        "    Args:\n",
        "        model: Keras model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with model info\n",
        "    \"\"\"\n",
        "    trainable_params = sum([tf.reduce_prod(w.shape).numpy() for w in model.trainable_weights])\n",
        "    non_trainable_params = sum([tf.reduce_prod(w.shape).numpy() for w in model.non_trainable_weights])\n",
        "\n",
        "    return {\n",
        "        'name': model.name,\n",
        "        'total_params': trainable_params + non_trainable_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'non_trainable_params': non_trainable_params,\n",
        "        'layers': len(model.layers)\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test model creation\n",
        "    print(\"Testing Scratch CNN...\")\n",
        "    scratch_model = create_scratch_cnn()\n",
        "    compile_model(scratch_model, config.SCRATCH_LR)\n",
        "\n",
        "    print(\"\\nTesting MobileNetV2...\")\n",
        "    mobilenet_model, mobilenet_base = create_mobilenet_model()\n",
        "    compile_model(mobilenet_model, config.TL_FREEZE_LR)\n",
        "\n",
        "    print(\"\\nTesting EfficientNetB0...\")\n",
        "    efficientnet_model, efficientnet_base = create_efficientnet_model()\n",
        "    compile_model(efficientnet_model, config.TL_FREEZE_LR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt3IR_PQT32-",
        "outputId": "356164d9-6214-435b-eab9-1f9f5f2f16f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/training.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Training Module\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "from typing import Optional, Tuple, Dict\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from . import config\n",
        "\n",
        "\n",
        "def get_callbacks(\n",
        "    model_name: str,\n",
        "    monitor: str = 'val_loss',\n",
        "    patience_early: int = None,\n",
        "    patience_lr: int = None,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Create training callbacks.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name for checkpoint file\n",
        "        monitor: Metric to monitor\n",
        "        patience_early: EarlyStopping patience\n",
        "        patience_lr: ReduceLROnPlateau patience\n",
        "\n",
        "    Returns:\n",
        "        List of callbacks\n",
        "    \"\"\"\n",
        "    if patience_early is None:\n",
        "        patience_early = config.EARLY_STOPPING_PATIENCE\n",
        "    if patience_lr is None:\n",
        "        patience_lr = config.REDUCE_LR_PATIENCE\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=monitor,\n",
        "            patience=patience_early,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=monitor,\n",
        "            factor=config.REDUCE_LR_FACTOR,\n",
        "            patience=patience_lr,\n",
        "            min_lr=config.MIN_LR,\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            filepath=os.path.join(config.MODELS_DIR, f'{model_name}_best.keras'),\n",
        "            monitor=monitor,\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model: keras.Model,\n",
        "    train_ds: tf.data.Dataset,\n",
        "    val_ds: tf.data.Dataset,\n",
        "    epochs: int = None,\n",
        "    callbacks: list = None,\n",
        "    verbose: int = 1\n",
        ") -> keras.callbacks.History:\n",
        "    \"\"\"\n",
        "    Train a model.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled Keras model\n",
        "        train_ds: Training dataset\n",
        "        val_ds: Validation dataset\n",
        "        epochs: Number of epochs\n",
        "        callbacks: List of callbacks\n",
        "        verbose: Verbosity level\n",
        "\n",
        "    Returns:\n",
        "        Training history\n",
        "    \"\"\"\n",
        "    if epochs is None:\n",
        "        epochs = config.EPOCHS\n",
        "\n",
        "    if callbacks is None:\n",
        "        callbacks = get_callbacks(model.name)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {model.name}\")\n",
        "    print(f\"Epochs: {epochs}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_training_history(\n",
        "    history: keras.callbacks.History,\n",
        "    model_name: str,\n",
        "    save_path: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss/accuracy curves.\n",
        "\n",
        "    Args:\n",
        "        history: Training history object\n",
        "        model_name: Model name for title\n",
        "        save_path: Path to save figure\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0].set_title(f'{model_name} - Loss', fontsize=12)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    axes[1].set_title(f'{model_name} - Accuracy', fontsize=12)\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Training curves saved to: {save_path}\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_history(history: keras.callbacks.History, model_name: str):\n",
        "    \"\"\"\n",
        "    Save training history to JSON file.\n",
        "\n",
        "    Args:\n",
        "        history: Training history\n",
        "        model_name: Model name\n",
        "    \"\"\"\n",
        "    history_dict = {k: [float(v) for v in vals] for k, vals in history.history.items()}\n",
        "\n",
        "    save_path = os.path.join(config.REPORTS_DIR, f'{model_name}_history.json')\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(history_dict, f, indent=2)\n",
        "\n",
        "    print(f\"Training history saved to: {save_path}\")\n",
        "\n",
        "\n",
        "def train_scratch_cnn(\n",
        "    train_ds: tf.data.Dataset,\n",
        "    val_ds: tf.data.Dataset,\n",
        "    epochs: int = None\n",
        ") -> Tuple[keras.Model, keras.callbacks.History]:\n",
        "    \"\"\"\n",
        "    Train the Scratch CNN model.\n",
        "\n",
        "    Args:\n",
        "        train_ds: Training dataset\n",
        "        val_ds: Validation dataset\n",
        "        epochs: Number of epochs\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (trained model, history)\n",
        "    \"\"\"\n",
        "    from .models import create_scratch_cnn, compile_model\n",
        "\n",
        "    model = create_scratch_cnn()\n",
        "    model = compile_model(model, config.SCRATCH_LR)\n",
        "\n",
        "    callbacks = get_callbacks('scratch_cnn')\n",
        "    history = train_model(model, train_ds, val_ds, epochs, callbacks)\n",
        "\n",
        "    # Save artifacts\n",
        "    plot_training_history(\n",
        "        history, 'Scratch CNN',\n",
        "        os.path.join(config.FIGURES_DIR, 'scratch_cnn_training.png')\n",
        "    )\n",
        "    save_history(history, 'scratch_cnn')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def train_mobilenet(\n",
        "    train_ds: tf.data.Dataset,\n",
        "    val_ds: tf.data.Dataset,\n",
        "    epochs_freeze: int = None,\n",
        "    epochs_finetune: int = None\n",
        ") -> Tuple[keras.Model, Dict]:\n",
        "    \"\"\"\n",
        "    Train MobileNetV2 with freeze + fine-tune strategy.\n",
        "\n",
        "    Args:\n",
        "        train_ds: Training dataset\n",
        "        val_ds: Validation dataset\n",
        "        epochs_freeze: Epochs for freeze phase\n",
        "        epochs_finetune: Epochs for fine-tune phase\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (trained model, histories dict)\n",
        "    \"\"\"\n",
        "    from .models import create_mobilenet_model, compile_model, unfreeze_top_layers\n",
        "\n",
        "    if epochs_freeze is None:\n",
        "        epochs_freeze = config.EPOCHS\n",
        "    if epochs_finetune is None:\n",
        "        epochs_finetune = config.EPOCHS\n",
        "\n",
        "    histories = {}\n",
        "\n",
        "    # Phase 1: Feature Extraction (Freeze)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MobileNetV2 - Phase 1: Feature Extraction (Freeze)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model, base_model = create_mobilenet_model(trainable=False)\n",
        "    model = compile_model(model, config.TL_FREEZE_LR)\n",
        "\n",
        "    callbacks = get_callbacks('mobilenet_freeze')\n",
        "    history_freeze = train_model(model, train_ds, val_ds, epochs_freeze, callbacks)\n",
        "    histories['freeze'] = history_freeze\n",
        "\n",
        "    plot_training_history(\n",
        "        history_freeze, 'MobileNetV2 - Freeze Phase',\n",
        "        os.path.join(config.FIGURES_DIR, 'mobilenet_freeze_training.png')\n",
        "    )\n",
        "    save_history(history_freeze, 'mobilenet_freeze')\n",
        "\n",
        "    # Phase 2: Fine-Tuning (Unfreeze top 25%)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MobileNetV2 - Phase 2: Fine-Tuning (Top 25%)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    unfreeze_top_layers(base_model, config.FINETUNE_RATIO)\n",
        "    model = compile_model(model, config.TL_FINETUNE_LR, show_summary=False)\n",
        "\n",
        "    callbacks = get_callbacks('mobilenet_finetune')\n",
        "    history_finetune = train_model(model, train_ds, val_ds, epochs_finetune, callbacks)\n",
        "    histories['finetune'] = history_finetune\n",
        "\n",
        "    plot_training_history(\n",
        "        history_finetune, 'MobileNetV2 - Fine-Tune Phase',\n",
        "        os.path.join(config.FIGURES_DIR, 'mobilenet_finetune_training.png')\n",
        "    )\n",
        "    save_history(history_finetune, 'mobilenet_finetune')\n",
        "\n",
        "    # Save final model\n",
        "    model.save(os.path.join(config.MODELS_DIR, 'mobilenet_final.keras'))\n",
        "\n",
        "    return model, histories\n",
        "\n",
        "\n",
        "def train_efficientnet(\n",
        "    train_ds: tf.data.Dataset,\n",
        "    val_ds: tf.data.Dataset,\n",
        "    epochs_freeze: int = None,\n",
        "    epochs_finetune: int = None\n",
        ") -> Tuple[keras.Model, Dict]:\n",
        "    \"\"\"\n",
        "    Train EfficientNetB0 with freeze + fine-tune strategy.\n",
        "\n",
        "    Args:\n",
        "        train_ds: Training dataset\n",
        "        val_ds: Validation dataset\n",
        "        epochs_freeze: Epochs for freeze phase\n",
        "        epochs_finetune: Epochs for fine-tune phase\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (trained model, histories dict)\n",
        "    \"\"\"\n",
        "    from .models import create_efficientnet_model, compile_model, unfreeze_top_layers\n",
        "\n",
        "    if epochs_freeze is None:\n",
        "        epochs_freeze = config.EPOCHS\n",
        "    if epochs_finetune is None:\n",
        "        epochs_finetune = config.EPOCHS\n",
        "\n",
        "    histories = {}\n",
        "\n",
        "    # Phase 1: Feature Extraction (Freeze)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EfficientNetB0 - Phase 1: Feature Extraction (Freeze)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model, base_model = create_efficientnet_model(trainable=False)\n",
        "    model = compile_model(model, config.TL_FREEZE_LR)\n",
        "\n",
        "    callbacks = get_callbacks('efficientnet_freeze')\n",
        "    history_freeze = train_model(model, train_ds, val_ds, epochs_freeze, callbacks)\n",
        "    histories['freeze'] = history_freeze\n",
        "\n",
        "    plot_training_history(\n",
        "        history_freeze, 'EfficientNetB0 - Freeze Phase',\n",
        "        os.path.join(config.FIGURES_DIR, 'efficientnet_freeze_training.png')\n",
        "    )\n",
        "    save_history(history_freeze, 'efficientnet_freeze')\n",
        "\n",
        "    # Phase 2: Fine-Tuning (Unfreeze top 25%)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EfficientNetB0 - Phase 2: Fine-Tuning (Top 25%)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    unfreeze_top_layers(base_model, config.FINETUNE_RATIO)\n",
        "    model = compile_model(model, config.TL_FINETUNE_LR, show_summary=False)\n",
        "\n",
        "    callbacks = get_callbacks('efficientnet_finetune')\n",
        "    history_finetune = train_model(model, train_ds, val_ds, epochs_finetune, callbacks)\n",
        "    histories['finetune'] = history_finetune\n",
        "\n",
        "    plot_training_history(\n",
        "        history_finetune, 'EfficientNetB0 - Fine-Tune Phase',\n",
        "        os.path.join(config.FIGURES_DIR, 'efficientnet_finetune_training.png')\n",
        "    )\n",
        "    save_history(history_finetune, 'efficientnet_finetune')\n",
        "\n",
        "    # Save final model\n",
        "    model.save(os.path.join(config.MODELS_DIR, 'efficientnet_final.keras'))\n",
        "\n",
        "    return model, histories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrVCeMO2T33A",
        "outputId": "84727952-0c46-4697-ad31-d896140c243d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/evaluation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/evaluation.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Evaluation Module\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "from . import config\n",
        "\n",
        "\n",
        "def predict_on_dataset(\n",
        "    model: keras.Model,\n",
        "    dataset: tf.data.Dataset\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Get predictions and true labels from a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: tf.data.Dataset\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (y_true, y_pred_proba)\n",
        "    \"\"\"\n",
        "    y_true = []\n",
        "    y_pred_proba = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        preds = model.predict(images, verbose=0)\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred_proba.extend(preds.flatten())\n",
        "\n",
        "    return np.array(y_true), np.array(y_pred_proba)\n",
        "\n",
        "\n",
        "def calculate_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred_proba: np.ndarray,\n",
        "    threshold: float = 0.5\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate classification metrics.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred_proba: Predicted probabilities\n",
        "        threshold: Classification threshold\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred_proba: np.ndarray,\n",
        "    class_names: Tuple[str, str],\n",
        "    model_name: str,\n",
        "    save_path: str = None,\n",
        "    threshold: float = 0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred_proba: Predicted probabilities\n",
        "        class_names: Tuple of class names\n",
        "        model_name: Model name for title\n",
        "        save_path: Path to save figure\n",
        "        threshold: Classification threshold\n",
        "    \"\"\"\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Confusion matrix saved to: {save_path}\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_roc_curve(\n",
        "    results: Dict[str, Tuple[np.ndarray, np.ndarray]],\n",
        "    save_path: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for multiple models.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of model_name -> (y_true, y_pred_proba)\n",
        "        save_path: Path to save figure\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "    for i, (model_name, (y_true, y_pred_proba)) in enumerate(results.items()):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "        auc = roc_auc_score(y_true, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, color=colors[i], linewidth=2,\n",
        "                label=f'{model_name} (AUC = {auc:.4f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
        "    plt.legend(loc='lower right', fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"ROC curves saved to: {save_path}\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def create_comparison_table(\n",
        "    metrics_dict: Dict[str, Dict[str, float]],\n",
        "    save_path: str = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a comparison table of metrics.\n",
        "\n",
        "    Args:\n",
        "        metrics_dict: Dict of model_name -> metrics\n",
        "        save_path: Path to save CSV\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with comparison\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(metrics_dict).T\n",
        "    df.index.name = 'Model'\n",
        "    df = df.round(4)\n",
        "\n",
        "    # Reorder columns\n",
        "    col_order = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "    df = df[col_order]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL COMPARISON - TEST SET METRICS\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.to_string())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Highlight best model\n",
        "    best_f1 = df['f1_score'].idxmax()\n",
        "    best_auc = df['roc_auc'].idxmax()\n",
        "    print(f\"\\nBest F1-Score: {best_f1} ({df.loc[best_f1, 'f1_score']:.4f})\")\n",
        "    print(f\"Best ROC-AUC: {best_auc} ({df.loc[best_auc, 'roc_auc']:.4f})\")\n",
        "\n",
        "    if save_path:\n",
        "        df.to_csv(save_path)\n",
        "        print(f\"\\nComparison table saved to: {save_path}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def print_classification_report(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred_proba: np.ndarray,\n",
        "    class_names: Tuple[str, str],\n",
        "    model_name: str,\n",
        "    threshold: float = 0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Print detailed classification report.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred_proba: Predicted probabilities\n",
        "        class_names: Tuple of class names\n",
        "        model_name: Model name\n",
        "        threshold: Classification threshold\n",
        "    \"\"\"\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Classification Report - {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_true, y_pred, target_names=list(class_names)))\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: keras.Model,\n",
        "    test_ds: tf.data.Dataset,\n",
        "    class_names: Tuple[str, str],\n",
        "    model_name: str\n",
        ") -> Tuple[Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Complete evaluation of a model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_ds: Test dataset\n",
        "        class_names: Tuple of class names\n",
        "        model_name: Model name\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (metrics, y_true, y_pred_proba)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get predictions\n",
        "    y_true, y_pred_proba = predict_on_dataset(model, test_ds)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(y_true, y_pred_proba)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(\n",
        "        y_true, y_pred_proba, class_names, model_name,\n",
        "        save_path=os.path.join(config.FIGURES_DIR, f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png')\n",
        "    )\n",
        "\n",
        "    # Print classification report\n",
        "    print_classification_report(y_true, y_pred_proba, class_names, model_name)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nMetrics Summary:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "\n",
        "    return metrics, y_true, y_pred_proba\n",
        "\n",
        "\n",
        "def evaluate_all_models(\n",
        "    models: Dict[str, keras.Model],\n",
        "    test_ds: tf.data.Dataset,\n",
        "    class_names: Tuple[str, str]\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Evaluate all models and create comparison.\n",
        "\n",
        "    Args:\n",
        "        models: Dict of model_name -> model\n",
        "        test_ds: Test dataset\n",
        "        class_names: Tuple of class names\n",
        "\n",
        "    Returns:\n",
        "        Dict of model_name -> metrics\n",
        "    \"\"\"\n",
        "    all_metrics = {}\n",
        "    all_predictions = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        metrics, y_true, y_pred_proba = evaluate_model(\n",
        "            model, test_ds, class_names, model_name\n",
        "        )\n",
        "        all_metrics[model_name] = metrics\n",
        "        all_predictions[model_name] = (y_true, y_pred_proba)\n",
        "\n",
        "    # Plot combined ROC curves\n",
        "    plot_roc_curve(\n",
        "        all_predictions,\n",
        "        save_path=os.path.join(config.FIGURES_DIR, 'roc_curves_comparison.png')\n",
        "    )\n",
        "\n",
        "    # Create comparison table\n",
        "    comparison_df = create_comparison_table(\n",
        "        all_metrics,\n",
        "        save_path=os.path.join(config.REPORTS_DIR, 'comparison_table.csv')\n",
        "    )\n",
        "\n",
        "    # Save all predictions for potential later use\n",
        "    for model_name, (y_true, y_pred_proba) in all_predictions.items():\n",
        "        save_path = os.path.join(config.REPORTS_DIR, f'{model_name.lower().replace(\" \", \"_\")}_predictions.npz')\n",
        "        np.savez(save_path, y_true=y_true, y_pred_proba=y_pred_proba)\n",
        "\n",
        "    return all_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I14BGvkAT33C",
        "outputId": "8f9cf24f-65a2-4f6a-91b9-93c3660078e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/gradcam.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/gradcam.py\n",
        "\"\"\"\n",
        "ISIC 2018 Skin Lesion Classification - Grad-CAM Module\n",
        "\"\"\"\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from . import config\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(\n",
        "    img_array: np.ndarray,\n",
        "    model: keras.Model,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM heatmap for an image.\n",
        "    Works with both regular CNN and transfer learning models.\n",
        "\n",
        "    Args:\n",
        "        img_array: Preprocessed image array (1, H, W, C)\n",
        "        model: Trained model\n",
        "\n",
        "    Returns:\n",
        "        Heatmap array\n",
        "    \"\"\"\n",
        "    # Find the layer before GlobalAveragePooling\n",
        "    gap_idx = None\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, keras.layers.GlobalAveragePooling2D):\n",
        "            gap_idx = i\n",
        "            break\n",
        "\n",
        "    if gap_idx is None or gap_idx == 0:\n",
        "        # Fallback: find any layer with spatial dimensions\n",
        "        for i, layer in enumerate(reversed(model.layers)):\n",
        "            if hasattr(layer, 'output') and len(layer.output.shape) == 4:\n",
        "                gap_idx = len(model.layers) - i\n",
        "                break\n",
        "\n",
        "    if gap_idx is None or gap_idx == 0:\n",
        "        raise ValueError(\"Could not find suitable layer for Grad-CAM\")\n",
        "\n",
        "    # Get the layer just before GAP\n",
        "    target_layer = model.layers[gap_idx - 1]\n",
        "\n",
        "    # Build activation model\n",
        "    activation_model = keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=[target_layer.output, model.output]\n",
        "    )\n",
        "\n",
        "    # Get activations and predictions\n",
        "    with tf.GradientTape() as tape:\n",
        "        activations, predictions = activation_model(img_array)\n",
        "        tape.watch(activations)\n",
        "        loss = predictions[:, 0]\n",
        "\n",
        "    # Get gradients\n",
        "    grads = tape.gradient(loss, activations)\n",
        "\n",
        "    if grads is None:\n",
        "        # Fallback: use activation magnitude\n",
        "        activations = activations[0]\n",
        "        heatmap = tf.reduce_mean(activations, axis=-1)\n",
        "    else:\n",
        "        # Weighted activation map\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "        activations = activations[0]\n",
        "        heatmap = activations @ pooled_grads[..., tf.newaxis]\n",
        "        heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # Normalize\n",
        "    heatmap = tf.maximum(heatmap, 0)\n",
        "    max_val = tf.math.reduce_max(heatmap)\n",
        "    if max_val > 0:\n",
        "        heatmap = heatmap / max_val\n",
        "\n",
        "    return heatmap.numpy()\n",
        "\n",
        "\n",
        "def create_gradcam_visualization(\n",
        "    image: np.ndarray,\n",
        "    heatmap: np.ndarray,\n",
        "    alpha: float = 0.4\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create overlay of Grad-CAM heatmap on original image.\n",
        "\n",
        "    Args:\n",
        "        image: Original image (H, W, C) in [0, 1]\n",
        "        heatmap: Grad-CAM heatmap\n",
        "        alpha: Overlay transparency\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (overlay, resized_heatmap)\n",
        "    \"\"\"\n",
        "    # Resize heatmap to image size\n",
        "    heatmap_resized = tf.image.resize(\n",
        "        heatmap[..., np.newaxis],\n",
        "        (image.shape[0], image.shape[1])\n",
        "    ).numpy().squeeze()\n",
        "\n",
        "    # Convert heatmap to RGB using colormap\n",
        "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]\n",
        "\n",
        "    # Create overlay\n",
        "    overlay = (1 - alpha) * image + alpha * heatmap_colored\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    return overlay, heatmap_resized\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load and preprocess a single image for Grad-CAM.\n",
        "\n",
        "    Args:\n",
        "        path: Image file path\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (original_image, preprocessed_image)\n",
        "    \"\"\"\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    # Keep original for visualization\n",
        "    original = tf.image.resize(img, config.IMG_SIZE).numpy() / 255.0\n",
        "\n",
        "    # Preprocess for model\n",
        "    preprocessed = tf.cast(tf.image.resize(img, config.IMG_SIZE), tf.float32) / 255.0\n",
        "    preprocessed = tf.expand_dims(preprocessed, 0)\n",
        "\n",
        "    return original, preprocessed.numpy()\n",
        "\n",
        "\n",
        "def select_samples_for_gradcam(\n",
        "    model: keras.Model,\n",
        "    test_paths: List[str],\n",
        "    test_labels: List[int],\n",
        "    num_correct: int = 3,\n",
        "    num_incorrect: int = 3\n",
        ") -> Tuple[List[dict], List[dict]]:\n",
        "    \"\"\"\n",
        "    Select samples for Grad-CAM analysis.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_paths: List of test image paths\n",
        "        test_labels: List of test labels\n",
        "        num_correct: Number of correctly classified samples\n",
        "        num_incorrect: Number of incorrectly classified samples\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (correct_samples, incorrect_samples)\n",
        "    \"\"\"\n",
        "    correct_samples = []\n",
        "    incorrect_samples = []\n",
        "\n",
        "    for path, true_label in zip(test_paths, test_labels):\n",
        "        if len(correct_samples) >= num_correct and len(incorrect_samples) >= num_incorrect:\n",
        "            break\n",
        "\n",
        "        # Load and predict\n",
        "        _, preprocessed = load_and_preprocess_image(path)\n",
        "        pred_proba = model.predict(preprocessed, verbose=0)[0, 0]\n",
        "        pred_label = int(pred_proba >= 0.5)\n",
        "\n",
        "        sample_info = {\n",
        "            'path': path,\n",
        "            'true_label': true_label,\n",
        "            'pred_label': pred_label,\n",
        "            'pred_proba': float(pred_proba),\n",
        "        }\n",
        "\n",
        "        if pred_label == true_label and len(correct_samples) < num_correct:\n",
        "            correct_samples.append(sample_info)\n",
        "        elif pred_label != true_label and len(incorrect_samples) < num_incorrect:\n",
        "            incorrect_samples.append(sample_info)\n",
        "\n",
        "    print(f\"Selected {len(correct_samples)} correct and {len(incorrect_samples)} incorrect samples\")\n",
        "\n",
        "    return correct_samples, incorrect_samples\n",
        "\n",
        "\n",
        "def generate_gradcam_for_samples(\n",
        "    model: keras.Model,\n",
        "    samples: List[dict],\n",
        "    class_names: Tuple[str, str],\n",
        "    model_name: str,\n",
        "    sample_type: str,\n",
        "    save_dir: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM visualizations for selected samples.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        samples: List of sample info dicts\n",
        "        class_names: Tuple of class names\n",
        "        model_name: Model name\n",
        "        sample_type: 'correct' or 'incorrect'\n",
        "        save_dir: Directory to save figures\n",
        "    \"\"\"\n",
        "    if save_dir is None:\n",
        "        save_dir = config.GRADCAM_DIR\n",
        "\n",
        "    print(f\"Generating Grad-CAM for {model_name} ({sample_type})\")\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        original, preprocessed = load_and_preprocess_image(sample['path'])\n",
        "\n",
        "        try:\n",
        "            # Generate heatmap\n",
        "            heatmap = make_gradcam_heatmap(preprocessed, model)\n",
        "\n",
        "            # Create overlay\n",
        "            overlay, heatmap_resized = create_gradcam_visualization(original, heatmap)\n",
        "\n",
        "            # Plot\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Original\n",
        "            axes[0].imshow(original)\n",
        "            axes[0].set_title('Original Image', fontsize=12)\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Heatmap\n",
        "            im = axes[1].imshow(heatmap_resized, cmap='jet')\n",
        "            axes[1].set_title('Grad-CAM Heatmap', fontsize=12)\n",
        "            axes[1].axis('off')\n",
        "            plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
        "\n",
        "            # Overlay\n",
        "            axes[2].imshow(overlay)\n",
        "            axes[2].set_title('Overlay', fontsize=12)\n",
        "            axes[2].axis('off')\n",
        "\n",
        "            # Title\n",
        "            true_class = class_names[sample['true_label']]\n",
        "            pred_class = class_names[sample['pred_label']]\n",
        "            confidence = sample['pred_proba'] if sample['pred_label'] == 1 else 1 - sample['pred_proba']\n",
        "\n",
        "            fig.suptitle(\n",
        "                f\"{model_name} - {sample_type.capitalize()}\\n\"\n",
        "                f\"True: {true_class} | Pred: {pred_class} (Conf: {confidence:.2%})\",\n",
        "                fontsize=14\n",
        "            )\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save\n",
        "            filename = f\"{model_name.lower().replace(' ', '_')}_{sample_type}_{i+1}.png\"\n",
        "            save_path = os.path.join(save_dir, filename)\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"  Saved: {filename}\")\n",
        "\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not generate Grad-CAM for sample {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "def generate_all_gradcam(\n",
        "    models: dict,\n",
        "    test_paths: List[str],\n",
        "    test_labels: List[int],\n",
        "    class_names: Tuple[str, str]\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM visualizations for all models.\n",
        "\n",
        "    Args:\n",
        "        models: Dict of model_name -> model\n",
        "        test_paths: List of test image paths\n",
        "        test_labels: List of test labels\n",
        "        class_names: Tuple of class names\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Generating Grad-CAM Visualizations\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nProcessing: {model_name}\")\n",
        "\n",
        "        # Select samples\n",
        "        correct_samples, incorrect_samples = select_samples_for_gradcam(\n",
        "            model, test_paths, test_labels,\n",
        "            num_correct=3, num_incorrect=3\n",
        "        )\n",
        "\n",
        "        # Generate visualizations\n",
        "        if correct_samples:\n",
        "            generate_gradcam_for_samples(\n",
        "                model, correct_samples, class_names, model_name, 'correct'\n",
        "            )\n",
        "\n",
        "        if incorrect_samples:\n",
        "            generate_gradcam_for_samples(\n",
        "                model, incorrect_samples, class_names, model_name, 'incorrect'\n",
        "            )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Grad-CAM generation complete!\")\n",
        "    print(f\"Visualizations saved to: {config.GRADCAM_DIR}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyax7HUqT33E"
      },
      "source": [
        "## 2. Pipeline Çalıştırma\n",
        "Artık tüm modüller hazır. Pipeline'ı başlatabiliriz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97ZwGQ9T33E",
        "outputId": "fce40be4-df7d-4e96-9c6a-3ca0d1ef0c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /kaggle/input/skin-cancer9-classesisic\n",
            "Contents: ['Skin cancer ISIC The International Skin Imaging Collaboration']\n",
            "Data root: /kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration\n",
            "Data root contents: ['Test', 'Train']\n",
            "\n",
            "TensorFlow Version: 2.19.0\n",
            "TRAIN_DIR: /kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train\n",
            "TEST_DIR: /kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Test\n",
            "TRAIN_DIR exists: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# src klasörünü path'e ekle\n",
        "sys.path.append('/content')\n",
        "\n",
        "from src import config\n",
        "from src.data_loader import prepare_data, visualize_augmentation\n",
        "from src.training import train_scratch_cnn, train_mobilenet, train_efficientnet\n",
        "from src.evaluation import evaluate_all_models\n",
        "from src.gradcam import generate_all_gradcam\n",
        "\n",
        "# kagglehub'dan indirilen veri seti yolunu kontrol et\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"Contents: {os.listdir(DATASET_PATH)}\")\n",
        "\n",
        "# Dizin yapısını otomatik tespit et\n",
        "def find_data_dirs(base_path):\n",
        "    '''Veri seti içindeki Train ve Test klasörlerini bul'''\n",
        "    # Doğrudan base_path'te Train var mı?\n",
        "    if os.path.isdir(os.path.join(base_path, 'Train')):\n",
        "        return base_path\n",
        "\n",
        "    # Alt klasörlerde ara\n",
        "    for item in os.listdir(base_path):\n",
        "        item_path = os.path.join(base_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            if os.path.isdir(os.path.join(item_path, 'Train')):\n",
        "                return item_path\n",
        "\n",
        "    # Bulunamadıysa base_path döndür\n",
        "    return base_path\n",
        "\n",
        "data_root = find_data_dirs(DATASET_PATH)\n",
        "print(f\"Data root: {data_root}\")\n",
        "print(f\"Data root contents: {os.listdir(data_root)}\")\n",
        "\n",
        "# config ayarlarını güncelle\n",
        "config.DATA_DIR = data_root\n",
        "config.TRAIN_DIR = os.path.join(data_root, 'Train')\n",
        "config.TEST_DIR = os.path.join(data_root, 'Test')\n",
        "\n",
        "print(f\"\\nTensorFlow Version: {tf.__version__}\")\n",
        "print(f\"TRAIN_DIR: {config.TRAIN_DIR}\")\n",
        "print(f\"TEST_DIR: {config.TEST_DIR}\")\n",
        "print(f\"TRAIN_DIR exists: {os.path.exists(config.TRAIN_DIR)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPN43zy0T33G",
        "outputId": "d2820910-739d-4a0f-99cc-bd21f3c9ea57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ISIC 2018 Dataset - Class Distribution\n",
            "============================================================\n",
            "                class_name  count\n",
            "pigmented benign keratosis    462\n",
            "                  melanoma    438\n",
            "      basal cell carcinoma    376\n",
            "                     nevus    357\n",
            "   squamous cell carcinoma    181\n",
            "           vascular lesion    139\n",
            "         actinic keratosis    114\n",
            "            dermatofibroma     95\n",
            "      seborrheic keratosis     77\n",
            "\n",
            "Total samples: 2239\n",
            "============================================================\n",
            "\n",
            "Selected classes for binary classification:\n",
            "  Class 0: pigmented benign keratosis\n",
            "  Class 1: melanoma\n",
            "\n",
            "Loaded 900 images:\n",
            "  Class 0 (pigmented benign keratosis): 462\n",
            "  Class 1 (melanoma): 438\n",
            "\n",
            "============================================================\n",
            "Stratified Split Results\n",
            "============================================================\n",
            "     TRAIN:  630 samples | Class 0: 324 | Class 1: 306\n",
            "       VAL:  135 samples | Class 0:  69 | Class 1:  66\n",
            "      TEST:  135 samples | Class 0:  69 | Class 1:  66\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [16.097439..16.981989].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [7.010318..7.893807].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [9.420583..10.179845].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [18.377863..19.174904].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [9.726975..10.533902].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation examples saved to: /content/outputs/figures/augmentation_examples.png\n"
          ]
        }
      ],
      "source": [
        "# Veri Hazırlığı\n",
        "train_ds, val_ds, test_ds, class_names, split_info = prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPKPF0g7T33H",
        "outputId": "f5145f69-65c1-4fae-b969-5edc16ce88fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [8.644585..9.251159].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [15.909157..16.8191].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [19.450615..20.122423].\n"
          ]
        }
      ],
      "source": [
        "# Augmentation Örnekleri\n",
        "visualize_augmentation(train_ds, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H2Q2alcmT33H"
      },
      "outputs": [],
      "source": [
        "# Model Eğitimi (Demo için epoch sayısını düşürebilirsiniz)\n",
        "config.EPOCHS = 10  # Colab'da daha hızlı sonuç için\n",
        "trained_models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7OTuc16QT33I",
        "outputId": "1bffdc93-f9d8-4d2d-94c4-22641dd54170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Scratch CNN...\n",
            "\n",
            "============================================================\n",
            "Model: ScratchCNN\n",
            "Learning Rate: 0.001\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ScratchCNN\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ScratchCNN\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_relu1 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_relu2 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_relu1 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_relu2 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_relu1 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_relu2 (\u001b[38;5;33mReLU\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_relu (\u001b[38;5;33mReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1_relu (\u001b[38;5;33mReLU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m618,017\u001b[0m (2.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">618,017</span> (2.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m616,609\u001b[0m (2.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">616,609</span> (2.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training: ScratchCNN\n",
            "Epochs: 10\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.5119 - auc: 0.5070 - loss: 0.8496 \n",
            "Epoch 1: val_loss improved from inf to 0.69543, saving model to /content/outputs/models/scratch_cnn_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 21s/step - accuracy: 0.5117 - auc: 0.5070 - loss: 0.8482 - val_accuracy: 0.3778 - val_auc: 0.3027 - val_loss: 0.6954 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.5039 - auc: 0.5158 - loss: 0.7391 \n",
            "Epoch 2: val_loss did not improve from 0.69543\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 20s/step - accuracy: 0.5035 - auc: 0.5150 - loss: 0.7384 - val_accuracy: 0.4889 - val_auc: 0.7048 - val_loss: 0.6965 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.5367 - auc: 0.5375 - loss: 0.6954 \n",
            "Epoch 3: val_loss improved from 0.69543 to 0.68949, saving model to /content/outputs/models/scratch_cnn_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 20s/step - accuracy: 0.5364 - auc: 0.5379 - loss: 0.6954 - val_accuracy: 0.5333 - val_auc: 0.6995 - val_loss: 0.6895 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.5149 - auc: 0.4665 - loss: 0.7249 \n",
            "Epoch 4: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 21s/step - accuracy: 0.5145 - auc: 0.4669 - loss: 0.7244 - val_accuracy: 0.4889 - val_auc: 0.6690 - val_loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.4971 - auc: 0.5288 - loss: 0.6934 \n",
            "Epoch 5: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 20s/step - accuracy: 0.4980 - auc: 0.5281 - loss: 0.6936 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6926 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - accuracy: 0.5152 - auc: 0.4897 - loss: 0.7023 \n",
            "Epoch 6: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 19s/step - accuracy: 0.5151 - auc: 0.4906 - loss: 0.7022 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6930 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.5254 - auc: 0.5154 - loss: 0.6926 \n",
            "Epoch 7: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 17s/step - accuracy: 0.5256 - auc: 0.5154 - loss: 0.6928 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.5089 - auc: 0.5284 - loss: 0.6952 \n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 17s/step - accuracy: 0.5078 - auc: 0.5270 - loss: 0.6956 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6930 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.5185 - auc: 0.5014 - loss: 0.6992 \n",
            "Epoch 9: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 17s/step - accuracy: 0.5184 - auc: 0.5013 - loss: 0.6992 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6929 - learning_rate: 2.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.5223 - auc: 0.5107 - loss: 0.6932 \n",
            "Epoch 10: val_loss did not improve from 0.68949\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 17s/step - accuracy: 0.5213 - auc: 0.5103 - loss: 0.6933 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6929 - learning_rate: 2.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Training curves saved to: /content/outputs/figures/scratch_cnn_training.png\n",
            "Training history saved to: /content/outputs/reports/scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Scratch CNN...\")\n",
        "scratch_model, scratch_history = train_scratch_cnn(train_ds, val_ds)\n",
        "trained_models['Scratch CNN'] = scratch_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XaVhDkM6T33I",
        "outputId": "ce94e6fd-f007-4317-b2fc-8cb8bad80321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MobileNetV2...\n",
            "\n",
            "============================================================\n",
            "MobileNetV2 - Phase 1: Feature Extraction (Freeze)\n",
            "============================================================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "============================================================\n",
            "Model: MobileNetV2_TL\n",
            "Learning Rate: 0.001\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"MobileNetV2_TL\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV2_TL\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ head_dropout (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ head_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training: MobileNetV2_TL\n",
            "Epochs: 10\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5102 - auc: 0.4992 - loss: 0.7565\n",
            "Epoch 1: val_loss improved from inf to 0.69959, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 0.5096 - auc: 0.4992 - loss: 0.7564 - val_accuracy: 0.4889 - val_auc: 0.5520 - val_loss: 0.6996 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4699 - auc: 0.4554 - loss: 0.7860\n",
            "Epoch 2: val_loss improved from 0.69959 to 0.69911, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.4713 - auc: 0.4572 - loss: 0.7846 - val_accuracy: 0.5111 - val_auc: 0.5747 - val_loss: 0.6991 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4906 - auc: 0.4628 - loss: 0.7713\n",
            "Epoch 3: val_loss improved from 0.69911 to 0.69357, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.4903 - auc: 0.4633 - loss: 0.7712 - val_accuracy: 0.5111 - val_auc: 0.5366 - val_loss: 0.6936 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4797 - auc: 0.4569 - loss: 0.7646\n",
            "Epoch 4: val_loss did not improve from 0.69357\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.4794 - auc: 0.4570 - loss: 0.7646 - val_accuracy: 0.4889 - val_auc: 0.5100 - val_loss: 0.6971 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5162 - auc: 0.5421 - loss: 0.7127\n",
            "Epoch 5: val_loss improved from 0.69357 to 0.69311, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.5161 - auc: 0.5420 - loss: 0.7130 - val_accuracy: 0.5111 - val_auc: 0.5688 - val_loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4924 - auc: 0.4981 - loss: 0.7304\n",
            "Epoch 6: val_loss did not improve from 0.69311\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.4926 - auc: 0.4988 - loss: 0.7301 - val_accuracy: 0.4815 - val_auc: 0.5019 - val_loss: 0.6934 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4947 - auc: 0.5297 - loss: 0.7188\n",
            "Epoch 7: val_loss improved from 0.69311 to 0.69300, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.4947 - auc: 0.5286 - loss: 0.7193 - val_accuracy: 0.5111 - val_auc: 0.5186 - val_loss: 0.6930 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5038 - auc: 0.5112 - loss: 0.7243\n",
            "Epoch 8: val_loss improved from 0.69300 to 0.69295, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.5043 - auc: 0.5116 - loss: 0.7240 - val_accuracy: 0.5111 - val_auc: 0.5782 - val_loss: 0.6929 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4927 - auc: 0.4769 - loss: 0.7373\n",
            "Epoch 9: val_loss improved from 0.69295 to 0.69294, saving model to /content/outputs/models/mobilenet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.4929 - auc: 0.4778 - loss: 0.7369 - val_accuracy: 0.5704 - val_auc: 0.4876 - val_loss: 0.6929 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4967 - auc: 0.4962 - loss: 0.7262\n",
            "Epoch 10: val_loss did not improve from 0.69294\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 4s/step - accuracy: 0.4974 - auc: 0.4972 - loss: 0.7260 - val_accuracy: 0.5111 - val_auc: 0.5610 - val_loss: 0.6969 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training curves saved to: /content/outputs/figures/mobilenet_freeze_training.png\n",
            "Training history saved to: /content/outputs/reports/mobilenet_freeze_history.json\n",
            "\n",
            "============================================================\n",
            "MobileNetV2 - Phase 2: Fine-Tuning (Top 25%)\n",
            "============================================================\n",
            "\n",
            "Fine-tuning configuration:\n",
            "  Total layers: 154\n",
            "  Frozen layers: 115\n",
            "  Unfrozen layers: 26\n",
            "  BatchNorm layers: kept frozen for stability\n",
            "\n",
            "============================================================\n",
            "Training: MobileNetV2_TL\n",
            "Epochs: 10\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5203 - auc: 0.5308 - loss: 0.7095\n",
            "Epoch 1: val_loss improved from inf to 0.69436, saving model to /content/outputs/models/mobilenet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.5189 - auc: 0.5291 - loss: 0.7102 - val_accuracy: 0.4889 - val_auc: 0.5036 - val_loss: 0.6944 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4652 - auc: 0.4640 - loss: 0.7137\n",
            "Epoch 2: val_loss improved from 0.69436 to 0.69305, saving model to /content/outputs/models/mobilenet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.4661 - auc: 0.4651 - loss: 0.7133 - val_accuracy: 0.5037 - val_auc: 0.5072 - val_loss: 0.6930 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5211 - auc: 0.5084 - loss: 0.6978\n",
            "Epoch 3: val_loss did not improve from 0.69305\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 0.5205 - auc: 0.5081 - loss: 0.6977 - val_accuracy: 0.4889 - val_auc: 0.5000 - val_loss: 0.6942 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5023 - auc: 0.4974 - loss: 0.6956\n",
            "Epoch 4: val_loss did not improve from 0.69305\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.5023 - auc: 0.4973 - loss: 0.6956 - val_accuracy: 0.4741 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4922 - auc: 0.5350 - loss: 0.6939\n",
            "Epoch 5: val_loss did not improve from 0.69305\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.4922 - auc: 0.5346 - loss: 0.6940 - val_accuracy: 0.5778 - val_auc: 0.4924 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5158 - auc: 0.4967 - loss: 0.6943\n",
            "Epoch 6: val_loss improved from 0.69305 to 0.69292, saving model to /content/outputs/models/mobilenet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 0.5154 - auc: 0.4956 - loss: 0.6944 - val_accuracy: 0.5111 - val_auc: 0.5227 - val_loss: 0.6929 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4998 - auc: 0.4878 - loss: 0.6943\n",
            "Epoch 7: val_loss did not improve from 0.69292\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3s/step - accuracy: 0.4989 - auc: 0.4868 - loss: 0.6944 - val_accuracy: 0.5111 - val_auc: 0.5145 - val_loss: 0.6930 - learning_rate: 1.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5259 - auc: 0.5068 - loss: 0.6928\n",
            "Epoch 8: val_loss did not improve from 0.69292\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 3s/step - accuracy: 0.5249 - auc: 0.5052 - loss: 0.6929 - val_accuracy: 0.5111 - val_auc: 0.5227 - val_loss: 0.6929 - learning_rate: 1.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4805 - auc: 0.4893 - loss: 0.6945\n",
            "Epoch 9: val_loss did not improve from 0.69292\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - accuracy: 0.4806 - auc: 0.4890 - loss: 0.6945 - val_accuracy: 0.4889 - val_auc: 0.5152 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4953 - auc: 0.5255 - loss: 0.6922\n",
            "Epoch 10: val_loss did not improve from 0.69292\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.4958 - auc: 0.5249 - loss: 0.6922 - val_accuracy: 0.5185 - val_auc: 0.4997 - val_loss: 0.6930 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Training curves saved to: /content/outputs/figures/mobilenet_finetune_training.png\n",
            "Training history saved to: /content/outputs/reports/mobilenet_finetune_history.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Training MobileNetV2...\")\n",
        "mobilenet_model, mobilenet_histories = train_mobilenet(train_ds, val_ds)\n",
        "trained_models['MobileNetV2'] = mobilenet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xEia4cLlT33J",
        "outputId": "27e055d5-9001-469b-ff88-17dbf7d13a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training EfficientNetB0...\n",
            "\n",
            "============================================================\n",
            "EfficientNetB0 - Phase 1: Feature Extraction (Freeze)\n",
            "============================================================\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "============================================================\n",
            "Model: EfficientNetB0_TL\n",
            "Learning Rate: 0.001\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"EfficientNetB0_TL\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB0_TL\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ head_dropout (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ head_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training: EfficientNetB0_TL\n",
            "Epochs: 10\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5130 - auc: 0.5078 - loss: 0.7001\n",
            "Epoch 1: val_loss improved from inf to 0.70199, saving model to /content/outputs/models/efficientnet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 4s/step - accuracy: 0.5121 - auc: 0.5064 - loss: 0.7005 - val_accuracy: 0.5111 - val_auc: 0.4621 - val_loss: 0.7020 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5204 - auc: 0.5013 - loss: 0.6997\n",
            "Epoch 2: val_loss improved from 0.70199 to 0.69311, saving model to /content/outputs/models/efficientnet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 4s/step - accuracy: 0.5196 - auc: 0.5007 - loss: 0.7000 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5442 - auc: 0.4702 - loss: 0.7034\n",
            "Epoch 3: val_loss did not improve from 0.69311\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4s/step - accuracy: 0.5432 - auc: 0.4716 - loss: 0.7033 - val_accuracy: 0.4889 - val_auc: 0.4470 - val_loss: 0.6970 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4859 - auc: 0.5107 - loss: 0.7014\n",
            "Epoch 4: val_loss did not improve from 0.69311\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 5s/step - accuracy: 0.4860 - auc: 0.5100 - loss: 0.7012 - val_accuracy: 0.5111 - val_auc: 0.4773 - val_loss: 0.6932 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4991 - auc: 0.5156 - loss: 0.6975\n",
            "Epoch 5: val_loss did not improve from 0.69311\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.4989 - auc: 0.5143 - loss: 0.6977 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6962 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5289 - auc: 0.5276 - loss: 0.6912\n",
            "Epoch 6: val_loss improved from 0.69311 to 0.69307, saving model to /content/outputs/models/efficientnet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.5282 - auc: 0.5270 - loss: 0.6914 - val_accuracy: 0.5111 - val_auc: 0.3271 - val_loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5125 - auc: 0.5188 - loss: 0.6953\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.69307\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.5120 - auc: 0.5174 - loss: 0.6955 - val_accuracy: 0.4889 - val_auc: 0.3195 - val_loss: 0.6937 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4638 - auc: 0.4663 - loss: 0.7052\n",
            "Epoch 8: val_loss did not improve from 0.69307\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4s/step - accuracy: 0.4639 - auc: 0.4663 - loss: 0.7050 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 2.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5428 - auc: 0.5491 - loss: 0.6887\n",
            "Epoch 9: val_loss improved from 0.69307 to 0.69304, saving model to /content/outputs/models/efficientnet_freeze_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4s/step - accuracy: 0.5419 - auc: 0.5479 - loss: 0.6890 - val_accuracy: 0.5111 - val_auc: 0.4848 - val_loss: 0.6930 - learning_rate: 2.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4862 - auc: 0.4969 - loss: 0.6973\n",
            "Epoch 10: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 5s/step - accuracy: 0.4862 - auc: 0.4963 - loss: 0.6975 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6930 - learning_rate: 2.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training curves saved to: /content/outputs/figures/efficientnet_freeze_training.png\n",
            "Training history saved to: /content/outputs/reports/efficientnet_freeze_history.json\n",
            "\n",
            "============================================================\n",
            "EfficientNetB0 - Phase 2: Fine-Tuning (Top 25%)\n",
            "============================================================\n",
            "\n",
            "Fine-tuning configuration:\n",
            "  Total layers: 238\n",
            "  Frozen layers: 178\n",
            "  Unfrozen layers: 47\n",
            "  BatchNorm layers: kept frozen for stability\n",
            "\n",
            "============================================================\n",
            "Training: EfficientNetB0_TL\n",
            "Epochs: 10\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5183 - auc: 0.5209 - loss: 0.6943\n",
            "Epoch 1: val_loss improved from inf to 0.69310, saving model to /content/outputs/models/efficientnet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 5s/step - accuracy: 0.5181 - auc: 0.5200 - loss: 0.6945 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4595 - auc: 0.4352 - loss: 0.7069\n",
            "Epoch 2: val_loss did not improve from 0.69310\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.4597 - auc: 0.4355 - loss: 0.7068 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6932 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5135 - auc: 0.4994 - loss: 0.6969\n",
            "Epoch 3: val_loss improved from 0.69310 to 0.69310, saving model to /content/outputs/models/efficientnet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 5s/step - accuracy: 0.5132 - auc: 0.4991 - loss: 0.6970 - val_accuracy: 0.5111 - val_auc: 0.4924 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4915 - auc: 0.5064 - loss: 0.6977\n",
            "Epoch 4: val_loss improved from 0.69310 to 0.69304, saving model to /content/outputs/models/efficientnet_finetune_best.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 5s/step - accuracy: 0.4930 - auc: 0.5076 - loss: 0.6974 - val_accuracy: 0.5111 - val_auc: 0.5010 - val_loss: 0.6930 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5022 - auc: 0.5224 - loss: 0.6957\n",
            "Epoch 5: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 5s/step - accuracy: 0.5026 - auc: 0.5226 - loss: 0.6955 - val_accuracy: 0.5111 - val_auc: 0.4470 - val_loss: 0.6931 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4967 - auc: 0.5252 - loss: 0.6918\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 5s/step - accuracy: 0.4972 - auc: 0.5244 - loss: 0.6920 - val_accuracy: 0.5111 - val_auc: 0.4924 - val_loss: 0.6933 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5319 - auc: 0.5460 - loss: 0.6945\n",
            "Epoch 7: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 5s/step - accuracy: 0.5303 - auc: 0.5438 - loss: 0.6946 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6932 - learning_rate: 2.0000e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5065 - auc: 0.5243 - loss: 0.6919\n",
            "Epoch 8: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 5s/step - accuracy: 0.5070 - auc: 0.5243 - loss: 0.6919 - val_accuracy: 0.5111 - val_auc: 0.5000 - val_loss: 0.6932 - learning_rate: 2.0000e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4932 - auc: 0.5004 - loss: 0.6977\n",
            "Epoch 9: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 5s/step - accuracy: 0.4934 - auc: 0.5002 - loss: 0.6976 - val_accuracy: 0.5111 - val_auc: 0.2991 - val_loss: 0.6931 - learning_rate: 2.0000e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4697 - auc: 0.4850 - loss: 0.6995\n",
            "Epoch 10: val_loss did not improve from 0.69304\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 5s/step - accuracy: 0.4711 - auc: 0.4856 - loss: 0.6993 - val_accuracy: 0.5111 - val_auc: 0.4621 - val_loss: 0.6931 - learning_rate: 2.0000e-06\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training curves saved to: /content/outputs/figures/efficientnet_finetune_training.png\n",
            "Training history saved to: /content/outputs/reports/efficientnet_finetune_history.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Training EfficientNetB0...\")\n",
        "efficientnet_model, efficientnet_histories = train_efficientnet(train_ds, val_ds)\n",
        "trained_models['EfficientNetB0'] = efficientnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB1-2CGFT33J",
        "outputId": "54077823-1111-4393-ad3a-0aed5b2ebba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Evaluating: Scratch CNN\n",
            "============================================================\n",
            "Confusion matrix saved to: /content/outputs/figures/scratch_cnn_confusion_matrix.png\n",
            "\n",
            "============================================================\n",
            "Classification Report - Scratch CNN\n",
            "============================================================\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "pigmented benign keratosis       0.53      1.00      0.69        69\n",
            "                  melanoma       1.00      0.06      0.11        66\n",
            "\n",
            "                  accuracy                           0.54       135\n",
            "                 macro avg       0.76      0.53      0.40       135\n",
            "              weighted avg       0.76      0.54      0.41       135\n",
            "\n",
            "\n",
            "Metrics Summary:\n",
            "  accuracy: 0.5407\n",
            "  precision: 1.0000\n",
            "  recall: 0.0606\n",
            "  f1_score: 0.1143\n",
            "  roc_auc: 0.7578\n",
            "\n",
            "============================================================\n",
            "Evaluating: MobileNetV2\n",
            "============================================================\n",
            "Confusion matrix saved to: /content/outputs/figures/mobilenetv2_confusion_matrix.png\n",
            "\n",
            "============================================================\n",
            "Classification Report - MobileNetV2\n",
            "============================================================\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "pigmented benign keratosis       0.51      1.00      0.68        69\n",
            "                  melanoma       0.00      0.00      0.00        66\n",
            "\n",
            "                  accuracy                           0.51       135\n",
            "                 macro avg       0.26      0.50      0.34       135\n",
            "              weighted avg       0.26      0.51      0.35       135\n",
            "\n",
            "\n",
            "Metrics Summary:\n",
            "  accuracy: 0.5111\n",
            "  precision: 0.0000\n",
            "  recall: 0.0000\n",
            "  f1_score: 0.0000\n",
            "  roc_auc: 0.5470\n",
            "\n",
            "============================================================\n",
            "Evaluating: EfficientNetB0\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c38081451c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c38081451c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix saved to: /content/outputs/figures/efficientnetb0_confusion_matrix.png\n",
            "\n",
            "============================================================\n",
            "Classification Report - EfficientNetB0\n",
            "============================================================\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "pigmented benign keratosis       0.51      1.00      0.68        69\n",
            "                  melanoma       0.00      0.00      0.00        66\n",
            "\n",
            "                  accuracy                           0.51       135\n",
            "                 macro avg       0.26      0.50      0.34       135\n",
            "              weighted avg       0.26      0.51      0.35       135\n",
            "\n",
            "\n",
            "Metrics Summary:\n",
            "  accuracy: 0.5111\n",
            "  precision: 0.0000\n",
            "  recall: 0.0000\n",
            "  f1_score: 0.0000\n",
            "  roc_auc: 0.3722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC curves saved to: /content/outputs/figures/roc_curves_comparison.png\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON - TEST SET METRICS\n",
            "================================================================================\n",
            "                accuracy  precision  recall  f1_score  roc_auc\n",
            "Model                                                         \n",
            "Scratch CNN       0.5407        1.0  0.0606    0.1143   0.7578\n",
            "MobileNetV2       0.5111        0.0  0.0000    0.0000   0.5470\n",
            "EfficientNetB0    0.5111        0.0  0.0000    0.0000   0.3722\n",
            "================================================================================\n",
            "\n",
            "Best F1-Score: Scratch CNN (0.1143)\n",
            "Best ROC-AUC: Scratch CNN (0.7578)\n",
            "\n",
            "Comparison table saved to: /content/outputs/reports/comparison_table.csv\n"
          ]
        }
      ],
      "source": [
        "# Değerlendirme\n",
        "results = evaluate_all_models(trained_models, test_ds, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvMAW8QqT33K",
        "outputId": "476b834d-3b12-4f5c-b39c-553a6a5a98b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Generating Grad-CAM Visualizations\n",
            "============================================================\n",
            "\n",
            "Processing: Scratch CNN\n",
            "Selected 3 correct and 3 incorrect samples\n",
            "Generating Grad-CAM for Scratch CNN (correct)\n",
            "  Saved: scratch_cnn_correct_1.png\n",
            "  Saved: scratch_cnn_correct_2.png\n",
            "  Saved: scratch_cnn_correct_3.png\n",
            "Generating Grad-CAM for Scratch CNN (incorrect)\n",
            "  Saved: scratch_cnn_incorrect_1.png\n",
            "  Saved: scratch_cnn_incorrect_2.png\n",
            "  Saved: scratch_cnn_incorrect_3.png\n",
            "\n",
            "Processing: MobileNetV2\n",
            "Selected 3 correct and 3 incorrect samples\n",
            "Generating Grad-CAM for MobileNetV2 (correct)\n",
            "  Warning: Could not generate Grad-CAM for sample 1: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.22324085, 0.11400279, 0.10778499],\\n         [0.22266187, 0.12767793, 0.10898891],\\n         [0.24550445, 0.13329145, 0.11406531],\\n         ...,\\n         [0.1765691 , 0.07525191, 0.07227662],\\n         [0.17064124, 0.07532391, 0.07438815],\\n         [0.16103783, 0.07190129, 0.06113735]],\\n\\n        [[0.219213  , 0.1167814 , 0.10840399],\\n         [0.23456852, 0.12163678, 0.11186787],\\n         [0.25042328, 0.12244398, 0.11578725],\\n         ...,\\n         [0.16294171, 0.07534382, 0.07957787],\\n         [0.16685897, 0.07150836, 0.07138741],\\n         [0.16368386, 0.06729688, 0.0695034 ]],\\n\\n        [[0.23567238, 0.1174823 , 0.10698874],\\n         [0.22648092, 0.14237726, 0.11681703],\\n         [0.23303634, 0.13153198, 0.1114499 ],\\n         ...,\\n         [0.15712437, 0.07983039, 0.06750727],\\n         [0.17336161, 0.07547981, 0.06674591],\\n         [0.16018224, 0.07813033, 0.07100808]],\\n\\n        ...,\\n\\n        [[0.16147089, 0.08243742, 0.08787481],\\n         [0.16647701, 0.07659335, 0.09092401],\\n         [0.16778615, 0.07920071, 0.09254046],\\n         ...,\\n         [0.11754067, 0.05035015, 0.04655011],\\n         [0.11218501, 0.03653003, 0.04076034],\\n         [0.10320736, 0.0291595 , 0.03482842]],\\n\\n        [[0.16197412, 0.08064631, 0.09070437],\\n         [0.17458358, 0.07064265, 0.09712292],\\n         [0.16428977, 0.07947144, 0.09838621],\\n         ...,\\n         [0.11136504, 0.03876348, 0.04204053],\\n         [0.1077906 , 0.03561074, 0.0410015 ],\\n         [0.10363367, 0.03658903, 0.0391275 ]],\\n\\n        [[0.17232434, 0.07806664, 0.09249241],\\n         [0.1724614 , 0.0767331 , 0.09227932],\\n         [0.16470589, 0.08161091, 0.09178585],\\n         ...,\\n         [0.11135854, 0.03330883, 0.04115197],\\n         [0.10505124, 0.03430485, 0.04024014],\\n         [0.09089015, 0.02970645, 0.03237324]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 2: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.5973677 , 0.46855682, 0.5558908 ],\\n         [0.5989768 , 0.47248024, 0.56670165],\\n         [0.5937112 , 0.47255963, 0.5635189 ],\\n         ...,\\n         [0.5993073 , 0.4737397 , 0.5326267 ],\\n         [0.60673136, 0.4832718 , 0.5431197 ],\\n         [0.5947303 , 0.4760419 , 0.53029037]],\\n\\n        [[0.59296876, 0.4695294 , 0.5582289 ],\\n         [0.59219193, 0.4657466 , 0.55397624],\\n         [0.592573  , 0.47241208, 0.559909  ],\\n         ...,\\n         [0.6003542 , 0.47490525, 0.52279806],\\n         [0.5910947 , 0.4676178 , 0.5226248 ],\\n         [0.59971863, 0.47593507, 0.5356955 ]],\\n\\n        [[0.5935258 , 0.46872216, 0.55484253],\\n         [0.5918179 , 0.46453393, 0.55871844],\\n         [0.59677684, 0.46706903, 0.55917275],\\n         ...,\\n         [0.5946597 , 0.4730918 , 0.53034043],\\n         [0.59913826, 0.4727625 , 0.53632015],\\n         [0.59621465, 0.47404394, 0.53440076]],\\n\\n        ...,\\n\\n        [[0.59527296, 0.45005605, 0.50288147],\\n         [0.5962599 , 0.45618725, 0.5049855 ],\\n         [0.6048317 , 0.47927266, 0.53438425],\\n         ...,\\n         [0.45248565, 0.3189252 , 0.39450353],\\n         [0.45359197, 0.30704   , 0.38758656],\\n         [0.44247195, 0.303981  , 0.37206584]],\\n\\n        [[0.5982052 , 0.44684032, 0.49868634],\\n         [0.59915185, 0.45405006, 0.5089733 ],\\n         [0.6059349 , 0.48119438, 0.5388934 ],\\n         ...,\\n         [0.45406136, 0.31920817, 0.38743597],\\n         [0.44231987, 0.31370577, 0.3832341 ],\\n         [0.43146074, 0.30460504, 0.37722346]],\\n\\n        [[0.59080875, 0.44837177, 0.49606085],\\n         [0.5962185 , 0.4509804 , 0.5041317 ],\\n         [0.6003677 , 0.4670344 , 0.5297795 ],\\n         ...,\\n         [0.46509823, 0.31922996, 0.3921289 ],\\n         [0.45382646, 0.31292954, 0.3871598 ],\\n         [0.4379848 , 0.308573  , 0.38840508]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 3: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.63526475, 0.61955535, 0.6572213 ],\\n         [0.6471282 , 0.6382787 , 0.6598993 ],\\n         [0.64825994, 0.64313036, 0.6474827 ],\\n         ...,\\n         [0.6592332 , 0.6160959 , 0.67884105],\\n         [0.6644962 , 0.62135893, 0.6762609 ],\\n         [0.67387956, 0.6307423 , 0.68626887]],\\n\\n        [[0.6365546 , 0.6271099 , 0.6499209 ],\\n         [0.64996904, 0.6382413 , 0.6606774 ],\\n         [0.645788  , 0.6363477 , 0.65994084],\\n         ...,\\n         [0.6553612 , 0.61826384, 0.67899567],\\n         [0.6675868 , 0.63048947, 0.6854632 ],\\n         [0.6684699 , 0.6313726 , 0.6842612 ]],\\n\\n        [[0.6291876 , 0.6232002 , 0.6349262 ],\\n         [0.6385608 , 0.6297238 , 0.6532594 ],\\n         [0.65563697, 0.6401611 , 0.67582226],\\n         ...,\\n         [0.6651957 , 0.62820345, 0.68890023],\\n         [0.6665646 , 0.62957233, 0.6884626 ],\\n         [0.66712177, 0.6301295 , 0.68298316]],\\n\\n        ...,\\n\\n        [[0.5158997 , 0.4432843 , 0.4825457 ],\\n         [0.50995654, 0.4392157 , 0.4626368 ],\\n         [0.50108606, 0.42246214, 0.42851943],\\n         ...,\\n         [0.6153536 , 0.56272715, 0.6297617 ],\\n         [0.6097163 , 0.5538267 , 0.6203473 ],\\n         [0.6029903 , 0.55200994, 0.5912256 ]],\\n\\n        [[0.51838243, 0.43201968, 0.47121787],\\n         [0.504046  , 0.42462268, 0.45714098],\\n         [0.51220554, 0.42350876, 0.46043417],\\n         ...,\\n         [0.6102383 , 0.562777  , 0.6207805 ],\\n         [0.6010758 , 0.55318606, 0.62077284],\\n         [0.59256864, 0.54255956, 0.5836659 ]],\\n\\n        [[0.5081787 , 0.42848682, 0.46962276],\\n         [0.50681925, 0.4302786 , 0.45990047],\\n         [0.5113585 , 0.4305462 , 0.46034983],\\n         ...,\\n         [0.6031303 , 0.55607146, 0.607087  ],\\n         [0.59418756, 0.54617476, 0.6019261 ],\\n         [0.5908087 , 0.5398283 , 0.5774131 ]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "Generating Grad-CAM for MobileNetV2 (incorrect)\n",
            "  Warning: Could not generate Grad-CAM for sample 1: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.8243697 , 0.602521  , 0.49411765],\\n         [0.8369748 , 0.62188876, 0.50394154],\\n         [0.8412365 , 0.6359144 , 0.50958383],\\n         ...,\\n         [0.8031413 , 0.6283514 , 0.49809927],\\n         [0.78517395, 0.6095437 , 0.4958182 ],\\n         [0.78039247, 0.614286  , 0.47619075]],\\n\\n        [[0.82997197, 0.62212884, 0.5005602 ],\\n         [0.8357943 , 0.6357943 , 0.51422566],\\n         [0.83763504, 0.6454782 , 0.51606643],\\n         ...,\\n         [0.7985194 , 0.64165664, 0.5044017 ],\\n         [0.7840933 , 0.623309  , 0.50566196],\\n         [0.77338964, 0.62437004, 0.48235294]],\\n\\n        [[0.8344538 , 0.6266107 , 0.4932773 ],\\n         [0.8352941 , 0.6392157 , 0.50980395],\\n         [0.8316726 , 0.6434373 , 0.50226086],\\n         ...,\\n         [0.7890359 , 0.6321732 , 0.4949183 ],\\n         [0.79169667, 0.6309123 , 0.51326525],\\n         [0.7926573 , 0.6240298 , 0.49685845]],\\n\\n        ...,\\n\\n        [[0.8177071 , 0.62555027, 0.5079032 ],\\n         [0.8184073 , 0.6210084 , 0.50598234],\\n         [0.81204474, 0.6188076 , 0.5017007 ],\\n         ...,\\n         [0.7647059 , 0.6       , 0.44313726],\\n         [0.7662063 , 0.60150045, 0.45248088],\\n         [0.7646459 , 0.5881753 , 0.4587635 ]],\\n\\n        [[0.80506194, 0.62074816, 0.5031011 ],\\n         [0.80856335, 0.6320928 , 0.51052415],\\n         [0.8100839 , 0.6336133 , 0.51204467],\\n         ...,\\n         [0.7689077 , 0.60420185, 0.44733912],\\n         [0.7626852 , 0.5979793 , 0.44895974],\\n         [0.7697279 , 0.5966385 , 0.46046433]],\\n\\n        [[0.8259503 , 0.62595034, 0.5043817 ],\\n         [0.80784315, 0.62352943, 0.5058824 ],\\n         [0.8216286 , 0.63115245, 0.5135054 ],\\n         ...,\\n         [0.77753115, 0.60498214, 0.45204097],\\n         [0.77537024, 0.6028212 , 0.44988006],\\n         [0.76554626, 0.58955574, 0.43847546]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 2: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.0047619 , 0.0047619 , 0.        ],\\n         [0.01206483, 0.01206483, 0.00422169],\\n         [0.01254502, 0.01254502, 0.00470188],\\n         ...,\\n         [0.0047619 , 0.0047619 , 0.0047619 ],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.00730292, 0.00730292, 0.        ],\\n         [0.01176471, 0.01176471, 0.00392157],\\n         [0.01802721, 0.01802721, 0.01018407],\\n         ...,\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00784314, 0.00784314, 0.00784314]],\\n\\n        [[0.00784314, 0.00784314, 0.        ],\\n         [0.01568628, 0.01568628, 0.00784314],\\n         [0.01932773, 0.01932773, 0.0114846 ],\\n         ...,\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00784314, 0.00784314, 0.00784314]],\\n\\n        ...,\\n\\n        [[0.01568628, 0.01568628, 0.00784314],\\n         [0.01540623, 0.02324937, 0.01148466],\\n         [0.01932766, 0.01932766, 0.01148453],\\n         ...,\\n         [0.0041816 , 0.0041816 , 0.0041816 ],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.01176471, 0.01176471, 0.00392157],\\n         [0.01568628, 0.02352941, 0.01176471],\\n         [0.01568628, 0.02352941, 0.01176471],\\n         ...,\\n         [0.00766297, 0.00766297, 0.00766297],\\n         [0.00252111, 0.00252111, 0.00252111],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.00850343, 0.00850343, 0.00066029],\\n         [0.00784314, 0.01568628, 0.00392157],\\n         [0.01646662, 0.02430976, 0.01254505],\\n         ...,\\n         [0.00470188, 0.00470188, 0.00470188],\\n         [0.00362146, 0.00362146, 0.00362146],\\n         [0.00392157, 0.00392157, 0.00392157]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 3: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580193644864\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.614846  , 0.4932773 , 0.42136857],\\n         [0.6153861 , 0.5055822 , 0.4232293 ],\\n         [0.620048  , 0.5102441 , 0.4239696 ],\\n         ...,\\n         [0.47326955, 0.32481053, 0.27354985],\\n         [0.48931548, 0.36976787, 0.3168066 ],\\n         [0.47657   , 0.34939915, 0.30226073]],\\n\\n        [[0.61736697, 0.49579832, 0.41736695],\\n         [0.62212884, 0.5123249 , 0.429972  ],\\n         [0.62727094, 0.517467  , 0.4311925 ],\\n         ...,\\n         [0.50274193, 0.36156544, 0.31450662],\\n         [0.49445742, 0.36112407, 0.31406525],\\n         [0.4675274 , 0.32188895, 0.28659484]],\\n\\n        [[0.6226291 , 0.5010604 , 0.42262906],\\n         [0.62342936, 0.51362544, 0.42735094],\\n         [0.63111246, 0.5213085 , 0.43503404],\\n         ...,\\n         [0.49773893, 0.36440557, 0.3251899 ],\\n         [0.48281312, 0.33379352, 0.2984994 ],\\n         [0.48179242, 0.32492968, 0.29355714]],\\n\\n        ...,\\n\\n        [[0.614846  , 0.5089636 , 0.42661065],\\n         [0.61904776, 0.5131654 , 0.43081245],\\n         [0.6235294 , 0.52549016, 0.43921563],\\n         ...,\\n         [0.54453725, 0.4112039 , 0.36414507],\\n         [0.5310923 , 0.38599423, 0.3507001 ],\\n         [0.523929  , 0.378831  , 0.34353688]],\\n\\n        [[0.60312104, 0.49723867, 0.41488573],\\n         [0.61478597, 0.5089036 , 0.42655066],\\n         [0.62717086, 0.5212885 , 0.43893558],\\n         ...,\\n         [0.5406159 , 0.40728256, 0.36806688],\\n         [0.5280111 , 0.38291302, 0.3476189 ],\\n         [0.52464956, 0.37170842, 0.34425744]],\\n\\n        [[0.60260105, 0.4895358 , 0.41696674],\\n         [0.612365  , 0.5025611 , 0.42020813],\\n         [0.6126051 , 0.50364155, 0.42128858],\\n         ...,\\n         [0.5476187 , 0.40812293, 0.3610641 ],\\n         [0.52743095, 0.38233286, 0.34703875],\\n         [0.5199883 , 0.36536586, 0.3387555 ]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "\n",
            "Processing: EfficientNetB0\n",
            "Selected 3 correct and 3 incorrect samples\n",
            "Generating Grad-CAM for EfficientNetB0 (correct)\n",
            "  Warning: Could not generate Grad-CAM for sample 1: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.22324085, 0.11400279, 0.10778499],\\n         [0.22266187, 0.12767793, 0.10898891],\\n         [0.24550445, 0.13329145, 0.11406531],\\n         ...,\\n         [0.1765691 , 0.07525191, 0.07227662],\\n         [0.17064124, 0.07532391, 0.07438815],\\n         [0.16103783, 0.07190129, 0.06113735]],\\n\\n        [[0.219213  , 0.1167814 , 0.10840399],\\n         [0.23456852, 0.12163678, 0.11186787],\\n         [0.25042328, 0.12244398, 0.11578725],\\n         ...,\\n         [0.16294171, 0.07534382, 0.07957787],\\n         [0.16685897, 0.07150836, 0.07138741],\\n         [0.16368386, 0.06729688, 0.0695034 ]],\\n\\n        [[0.23567238, 0.1174823 , 0.10698874],\\n         [0.22648092, 0.14237726, 0.11681703],\\n         [0.23303634, 0.13153198, 0.1114499 ],\\n         ...,\\n         [0.15712437, 0.07983039, 0.06750727],\\n         [0.17336161, 0.07547981, 0.06674591],\\n         [0.16018224, 0.07813033, 0.07100808]],\\n\\n        ...,\\n\\n        [[0.16147089, 0.08243742, 0.08787481],\\n         [0.16647701, 0.07659335, 0.09092401],\\n         [0.16778615, 0.07920071, 0.09254046],\\n         ...,\\n         [0.11754067, 0.05035015, 0.04655011],\\n         [0.11218501, 0.03653003, 0.04076034],\\n         [0.10320736, 0.0291595 , 0.03482842]],\\n\\n        [[0.16197412, 0.08064631, 0.09070437],\\n         [0.17458358, 0.07064265, 0.09712292],\\n         [0.16428977, 0.07947144, 0.09838621],\\n         ...,\\n         [0.11136504, 0.03876348, 0.04204053],\\n         [0.1077906 , 0.03561074, 0.0410015 ],\\n         [0.10363367, 0.03658903, 0.0391275 ]],\\n\\n        [[0.17232434, 0.07806664, 0.09249241],\\n         [0.1724614 , 0.0767331 , 0.09227932],\\n         [0.16470589, 0.08161091, 0.09178585],\\n         ...,\\n         [0.11135854, 0.03330883, 0.04115197],\\n         [0.10505124, 0.03430485, 0.04024014],\\n         [0.09089015, 0.02970645, 0.03237324]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 2: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.5973677 , 0.46855682, 0.5558908 ],\\n         [0.5989768 , 0.47248024, 0.56670165],\\n         [0.5937112 , 0.47255963, 0.5635189 ],\\n         ...,\\n         [0.5993073 , 0.4737397 , 0.5326267 ],\\n         [0.60673136, 0.4832718 , 0.5431197 ],\\n         [0.5947303 , 0.4760419 , 0.53029037]],\\n\\n        [[0.59296876, 0.4695294 , 0.5582289 ],\\n         [0.59219193, 0.4657466 , 0.55397624],\\n         [0.592573  , 0.47241208, 0.559909  ],\\n         ...,\\n         [0.6003542 , 0.47490525, 0.52279806],\\n         [0.5910947 , 0.4676178 , 0.5226248 ],\\n         [0.59971863, 0.47593507, 0.5356955 ]],\\n\\n        [[0.5935258 , 0.46872216, 0.55484253],\\n         [0.5918179 , 0.46453393, 0.55871844],\\n         [0.59677684, 0.46706903, 0.55917275],\\n         ...,\\n         [0.5946597 , 0.4730918 , 0.53034043],\\n         [0.59913826, 0.4727625 , 0.53632015],\\n         [0.59621465, 0.47404394, 0.53440076]],\\n\\n        ...,\\n\\n        [[0.59527296, 0.45005605, 0.50288147],\\n         [0.5962599 , 0.45618725, 0.5049855 ],\\n         [0.6048317 , 0.47927266, 0.53438425],\\n         ...,\\n         [0.45248565, 0.3189252 , 0.39450353],\\n         [0.45359197, 0.30704   , 0.38758656],\\n         [0.44247195, 0.303981  , 0.37206584]],\\n\\n        [[0.5982052 , 0.44684032, 0.49868634],\\n         [0.59915185, 0.45405006, 0.5089733 ],\\n         [0.6059349 , 0.48119438, 0.5388934 ],\\n         ...,\\n         [0.45406136, 0.31920817, 0.38743597],\\n         [0.44231987, 0.31370577, 0.3832341 ],\\n         [0.43146074, 0.30460504, 0.37722346]],\\n\\n        [[0.59080875, 0.44837177, 0.49606085],\\n         [0.5962185 , 0.4509804 , 0.5041317 ],\\n         [0.6003677 , 0.4670344 , 0.5297795 ],\\n         ...,\\n         [0.46509823, 0.31922996, 0.3921289 ],\\n         [0.45382646, 0.31292954, 0.3871598 ],\\n         [0.4379848 , 0.308573  , 0.38840508]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 3: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.63526475, 0.61955535, 0.6572213 ],\\n         [0.6471282 , 0.6382787 , 0.6598993 ],\\n         [0.64825994, 0.64313036, 0.6474827 ],\\n         ...,\\n         [0.6592332 , 0.6160959 , 0.67884105],\\n         [0.6644962 , 0.62135893, 0.6762609 ],\\n         [0.67387956, 0.6307423 , 0.68626887]],\\n\\n        [[0.6365546 , 0.6271099 , 0.6499209 ],\\n         [0.64996904, 0.6382413 , 0.6606774 ],\\n         [0.645788  , 0.6363477 , 0.65994084],\\n         ...,\\n         [0.6553612 , 0.61826384, 0.67899567],\\n         [0.6675868 , 0.63048947, 0.6854632 ],\\n         [0.6684699 , 0.6313726 , 0.6842612 ]],\\n\\n        [[0.6291876 , 0.6232002 , 0.6349262 ],\\n         [0.6385608 , 0.6297238 , 0.6532594 ],\\n         [0.65563697, 0.6401611 , 0.67582226],\\n         ...,\\n         [0.6651957 , 0.62820345, 0.68890023],\\n         [0.6665646 , 0.62957233, 0.6884626 ],\\n         [0.66712177, 0.6301295 , 0.68298316]],\\n\\n        ...,\\n\\n        [[0.5158997 , 0.4432843 , 0.4825457 ],\\n         [0.50995654, 0.4392157 , 0.4626368 ],\\n         [0.50108606, 0.42246214, 0.42851943],\\n         ...,\\n         [0.6153536 , 0.56272715, 0.6297617 ],\\n         [0.6097163 , 0.5538267 , 0.6203473 ],\\n         [0.6029903 , 0.55200994, 0.5912256 ]],\\n\\n        [[0.51838243, 0.43201968, 0.47121787],\\n         [0.504046  , 0.42462268, 0.45714098],\\n         [0.51220554, 0.42350876, 0.46043417],\\n         ...,\\n         [0.6102383 , 0.562777  , 0.6207805 ],\\n         [0.6010758 , 0.55318606, 0.62077284],\\n         [0.59256864, 0.54255956, 0.5836659 ]],\\n\\n        [[0.5081787 , 0.42848682, 0.46962276],\\n         [0.50681925, 0.4302786 , 0.45990047],\\n         [0.5113585 , 0.4305462 , 0.46034983],\\n         ...,\\n         [0.6031303 , 0.55607146, 0.607087  ],\\n         [0.59418756, 0.54617476, 0.6019261 ],\\n         [0.5908087 , 0.5398283 , 0.5774131 ]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "Generating Grad-CAM for EfficientNetB0 (incorrect)\n",
            "  Warning: Could not generate Grad-CAM for sample 1: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.8243697 , 0.602521  , 0.49411765],\\n         [0.8369748 , 0.62188876, 0.50394154],\\n         [0.8412365 , 0.6359144 , 0.50958383],\\n         ...,\\n         [0.8031413 , 0.6283514 , 0.49809927],\\n         [0.78517395, 0.6095437 , 0.4958182 ],\\n         [0.78039247, 0.614286  , 0.47619075]],\\n\\n        [[0.82997197, 0.62212884, 0.5005602 ],\\n         [0.8357943 , 0.6357943 , 0.51422566],\\n         [0.83763504, 0.6454782 , 0.51606643],\\n         ...,\\n         [0.7985194 , 0.64165664, 0.5044017 ],\\n         [0.7840933 , 0.623309  , 0.50566196],\\n         [0.77338964, 0.62437004, 0.48235294]],\\n\\n        [[0.8344538 , 0.6266107 , 0.4932773 ],\\n         [0.8352941 , 0.6392157 , 0.50980395],\\n         [0.8316726 , 0.6434373 , 0.50226086],\\n         ...,\\n         [0.7890359 , 0.6321732 , 0.4949183 ],\\n         [0.79169667, 0.6309123 , 0.51326525],\\n         [0.7926573 , 0.6240298 , 0.49685845]],\\n\\n        ...,\\n\\n        [[0.8177071 , 0.62555027, 0.5079032 ],\\n         [0.8184073 , 0.6210084 , 0.50598234],\\n         [0.81204474, 0.6188076 , 0.5017007 ],\\n         ...,\\n         [0.7647059 , 0.6       , 0.44313726],\\n         [0.7662063 , 0.60150045, 0.45248088],\\n         [0.7646459 , 0.5881753 , 0.4587635 ]],\\n\\n        [[0.80506194, 0.62074816, 0.5031011 ],\\n         [0.80856335, 0.6320928 , 0.51052415],\\n         [0.8100839 , 0.6336133 , 0.51204467],\\n         ...,\\n         [0.7689077 , 0.60420185, 0.44733912],\\n         [0.7626852 , 0.5979793 , 0.44895974],\\n         [0.7697279 , 0.5966385 , 0.46046433]],\\n\\n        [[0.8259503 , 0.62595034, 0.5043817 ],\\n         [0.80784315, 0.62352943, 0.5058824 ],\\n         [0.8216286 , 0.63115245, 0.5135054 ],\\n         ...,\\n         [0.77753115, 0.60498214, 0.45204097],\\n         [0.77537024, 0.6028212 , 0.44988006],\\n         [0.76554626, 0.58955574, 0.43847546]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 2: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.0047619 , 0.0047619 , 0.        ],\\n         [0.01206483, 0.01206483, 0.00422169],\\n         [0.01254502, 0.01254502, 0.00470188],\\n         ...,\\n         [0.0047619 , 0.0047619 , 0.0047619 ],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.00730292, 0.00730292, 0.        ],\\n         [0.01176471, 0.01176471, 0.00392157],\\n         [0.01802721, 0.01802721, 0.01018407],\\n         ...,\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00784314, 0.00784314, 0.00784314]],\\n\\n        [[0.00784314, 0.00784314, 0.        ],\\n         [0.01568628, 0.01568628, 0.00784314],\\n         [0.01932773, 0.01932773, 0.0114846 ],\\n         ...,\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00784314, 0.00784314, 0.00784314],\\n         [0.00784314, 0.00784314, 0.00784314]],\\n\\n        ...,\\n\\n        [[0.01568628, 0.01568628, 0.00784314],\\n         [0.01540623, 0.02324937, 0.01148466],\\n         [0.01932766, 0.01932766, 0.01148453],\\n         ...,\\n         [0.0041816 , 0.0041816 , 0.0041816 ],\\n         [0.00392157, 0.00392157, 0.00392157],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.01176471, 0.01176471, 0.00392157],\\n         [0.01568628, 0.02352941, 0.01176471],\\n         [0.01568628, 0.02352941, 0.01176471],\\n         ...,\\n         [0.00766297, 0.00766297, 0.00766297],\\n         [0.00252111, 0.00252111, 0.00252111],\\n         [0.00392157, 0.00392157, 0.00392157]],\\n\\n        [[0.00850343, 0.00850343, 0.00066029],\\n         [0.00784314, 0.01568628, 0.00392157],\\n         [0.01646662, 0.02430976, 0.01254505],\\n         ...,\\n         [0.00470188, 0.00470188, 0.00470188],\\n         [0.00362146, 0.00362146, 0.00362146],\\n         [0.00392157, 0.00392157, 0.00392157]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "  Warning: Could not generate Grad-CAM for sample 3: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136580440516960\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=array([[[[0.614846  , 0.4932773 , 0.42136857],\\n         [0.6153861 , 0.5055822 , 0.4232293 ],\\n         [0.620048  , 0.5102441 , 0.4239696 ],\\n         ...,\\n         [0.47326955, 0.32481053, 0.27354985],\\n         [0.48931548, 0.36976787, 0.3168066 ],\\n         [0.47657   , 0.34939915, 0.30226073]],\\n\\n        [[0.61736697, 0.49579832, 0.41736695],\\n         [0.62212884, 0.5123249 , 0.429972  ],\\n         [0.62727094, 0.517467  , 0.4311925 ],\\n         ...,\\n         [0.50274193, 0.36156544, 0.31450662],\\n         [0.49445742, 0.36112407, 0.31406525],\\n         [0.4675274 , 0.32188895, 0.28659484]],\\n\\n        [[0.6226291 , 0.5010604 , 0.42262906],\\n         [0.62342936, 0.51362544, 0.42735094],\\n         [0.63111246, 0.5213085 , 0.43503404],\\n         ...,\\n         [0.49773893, 0.36440557, 0.3251899 ],\\n         [0.48281312, 0.33379352, 0.2984994 ],\\n         [0.48179242, 0.32492968, 0.29355714]],\\n\\n        ...,\\n\\n        [[0.614846  , 0.5089636 , 0.42661065],\\n         [0.61904776, 0.5131654 , 0.43081245],\\n         [0.6235294 , 0.52549016, 0.43921563],\\n         ...,\\n         [0.54453725, 0.4112039 , 0.36414507],\\n         [0.5310923 , 0.38599423, 0.3507001 ],\\n         [0.523929  , 0.378831  , 0.34353688]],\\n\\n        [[0.60312104, 0.49723867, 0.41488573],\\n         [0.61478597, 0.5089036 , 0.42655066],\\n         [0.62717086, 0.5212885 , 0.43893558],\\n         ...,\\n         [0.5406159 , 0.40728256, 0.36806688],\\n         [0.5280111 , 0.38291302, 0.3476189 ],\\n         [0.52464956, 0.37170842, 0.34425744]],\\n\\n        [[0.60260105, 0.4895358 , 0.41696674],\\n         [0.612365  , 0.5025611 , 0.42020813],\\n         [0.6126051 , 0.50364155, 0.42128858],\\n         ...,\\n         [0.5476187 , 0.40812293, 0.3610641 ],\\n         [0.52743095, 0.38233286, 0.34703875],\\n         [0.5199883 , 0.36536586, 0.3387555 ]]]], dtype=float32)\\n  • training=None\\n  • mask=None\\n  • kwargs=<class 'inspect._empty'>\"\n",
            "\n",
            "============================================================\n",
            "Grad-CAM generation complete!\n",
            "Visualizations saved to: /content/outputs/gradcam\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Grad-CAM Analizi\n",
        "generate_all_gradcam(\n",
        "    trained_models,\n",
        "    split_info['test_paths'],\n",
        "    split_info['test_labels'],\n",
        "    class_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoIyoChT33K"
      },
      "source": [
        "## Sonuçlar\n",
        "Tüm görseller ve raporlar sol paneldeki `outputs` klasöründe bulunabilir. Bu klasörü sıkıştırıp indirebilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNHNambVT33K",
        "outputId": "94bbb153-d985-42b9-b30c-c74243f056f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: outputs/ (stored 0%)\n",
            "  adding: outputs/gradcam/ (stored 0%)\n",
            "  adding: outputs/gradcam/scratch_cnn_incorrect_2.png (deflated 1%)\n",
            "  adding: outputs/gradcam/scratch_cnn_correct_2.png (deflated 1%)\n",
            "  adding: outputs/gradcam/scratch_cnn_correct_3.png (deflated 1%)\n",
            "  adding: outputs/gradcam/scratch_cnn_incorrect_1.png (deflated 0%)\n",
            "  adding: outputs/gradcam/scratch_cnn_correct_1.png (deflated 1%)\n",
            "  adding: outputs/gradcam/scratch_cnn_incorrect_3.png (deflated 1%)\n",
            "  adding: outputs/reports/ (stored 0%)\n",
            "  adding: outputs/reports/mobilenet_finetune_history.json (deflated 68%)\n",
            "  adding: outputs/reports/mobilenet_freeze_history.json (deflated 68%)\n",
            "  adding: outputs/reports/efficientnetb0_predictions.npz (deflated 57%)\n",
            "  adding: outputs/reports/comparison_table.csv (deflated 25%)\n",
            "  adding: outputs/reports/mobilenetv2_predictions.npz (deflated 57%)\n",
            "  adding: outputs/reports/efficientnet_finetune_history.json (deflated 69%)\n",
            "  adding: outputs/reports/scratch_cnn_history.json (deflated 68%)\n",
            "  adding: outputs/reports/scratch_cnn_predictions.npz (deflated 53%)\n",
            "  adding: outputs/reports/efficientnet_freeze_history.json (deflated 69%)\n",
            "  adding: outputs/models/ (stored 0%)\n",
            "  adding: outputs/models/efficientnet_finetune_best.keras (deflated 9%)\n",
            "  adding: outputs/models/scratch_cnn_best.keras (deflated 9%)\n",
            "  adding: outputs/models/mobilenet_finetune_best.keras (deflated 14%)\n",
            "  adding: outputs/models/efficientnet_final.keras (deflated 9%)\n",
            "  adding: outputs/models/mobilenet_freeze_best.keras (deflated 12%)\n",
            "  adding: outputs/models/mobilenet_final.keras (deflated 14%)\n",
            "  adding: outputs/models/efficientnet_freeze_best.keras (deflated 12%)\n",
            "  adding: outputs/figures/ (stored 0%)\n",
            "  adding: outputs/figures/efficientnetb0_confusion_matrix.png (deflated 18%)\n",
            "  adding: outputs/figures/efficientnet_freeze_training.png (deflated 6%)\n",
            "  adding: outputs/figures/mobilenetv2_confusion_matrix.png (deflated 18%)\n",
            "  adding: outputs/figures/mobilenet_finetune_training.png (deflated 9%)\n",
            "  adding: outputs/figures/efficientnet_finetune_training.png (deflated 9%)\n",
            "  adding: outputs/figures/mobilenet_freeze_training.png (deflated 8%)\n",
            "  adding: outputs/figures/scratch_cnn_confusion_matrix.png (deflated 18%)\n",
            "  adding: outputs/figures/augmentation_examples.png (deflated 5%)\n",
            "  adding: outputs/figures/roc_curves_comparison.png (deflated 17%)\n",
            "  adding: outputs/figures/scratch_cnn_training.png (deflated 10%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r outputs.zip outputs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}